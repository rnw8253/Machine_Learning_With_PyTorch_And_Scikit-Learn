{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59943c35-3a8f-485b-80ac-4fdd12aab51c",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c2292c-d095-4288-b2e8-1d7a4fc5dc9c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Chapter 1: Introduction to Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556b17b5-5338-4a30-b44b-577dbad43f67",
   "metadata": {},
   "source": [
    "### <u> 1.1 Types of Machine Learning </u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725e6e29-c2fd-4ebd-a70d-7d8a23a76da7",
   "metadata": {},
   "source": [
    "- $\\textbf{Supervised Learning}$\n",
    "    - Labeled Data\n",
    "    - Direct Feedback\n",
    "    - Predict outcome/future\n",
    "    - Subcategories:\n",
    "        - Classification - predict categorical class\n",
    "        - Regression - predict continuous outcomes\n",
    "- $\\textbf{Unsupervised Learning}$\n",
    "    - No labels/targets\n",
    "    - No feedback\n",
    "    - Find hidden structure in data\n",
    "    - Subcategories:\n",
    "        - Clustering - organize data into meaningful subgroups\n",
    "        - Dimensionality Reduction - compress the data onto a smaller dimensional subspace while retaining most of the relevant information\n",
    "- $\\textbf{Reinforcement Learning}$\n",
    "    - Decision process\n",
    "    - Reward system\n",
    "    - Learn series of actions\n",
    "    - Agent improves its performance based on interactions with the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ed4c31-b083-476d-8538-b2f63ce0be4d",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Chapter 2: Simple Machine Learning Algorithms For Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b5a269-a43b-4290-8c6b-0f4b986c32e8",
   "metadata": {},
   "source": [
    "<img src=\"machineLearningRoadmap.png\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e3912c-6ed7-4f3e-a6f2-290fc7ca81f8",
   "metadata": {},
   "source": [
    "### <u> 2.1 Perceptron </u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a65a4b4-5012-46c4-810c-4630bbb69117",
   "metadata": {},
   "source": [
    "- Rosenblatt's thresholded perceptron model mimics the function of a single neuron in the brain\n",
    "- Use perceptron implementation to create a decision boundary to classify data into categories (binary)\n",
    "- $\\textbf{OvR}$: one-versus-rest - technique that allows us to extend any binary classifier to multi-class problems\n",
    "- Takes a combination of certain inputs and a corresponding weight vector to predict the outcome\n",
    "\n",
    "- Learning rule:\n",
    "    1. Initialize the weights and bias unit to 0 or small random numbers\n",
    "    2. For each training example, $x^{(i)}$:\n",
    "        - Compute the output value, $\\hat{y}^{(i)}$ (predicted class label)\n",
    "        - Update the weights and bias unit\n",
    "            - $w_{j} := w_{j} + \\Delta w_{j}$ where $\\Delta w_{j} = \\eta(y^{(i)} - \\hat{y}^{(i)})x_{j}^{(i)}$\n",
    "            - $b := b + \\Delta b$ where $\\Delta b = \\eta(y^{(i)} - \\hat{y}^{(i)})$\n",
    "            - $\\eta$ is the learning rate\n",
    "\n",
    "\n",
    "<img src=\"PerceptronWeightsAndBias.png\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95733e46-db43-4856-8353-3f2191898639",
   "metadata": {},
   "source": [
    "### <u> 2.2 Adaptive Linear Neurons (Adaline) </u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee647cf4-0178-41cb-aea8-a99f9f291d9b",
   "metadata": {},
   "source": [
    "<img src=\"AdalineWeightsAndBias.png\" width=\"50%\"/>\n",
    "\n",
    "- Weights are updated based on a linear activation function rather than a unit step function like in the perceptron\n",
    "- Threshold function is still used to make the final prediction\n",
    "- $\\textbf{Objective Function}$ - optimized during the learning process; often a loss or cost function that we want to minimize\n",
    "- Adaline loss function is the mean squared error (MSE) between the calculated outcome and the true class label:\n",
    "    - $L(w, b) = \\frac{1}{n} \\sum\\limits_{i=1}^{n} (y^{(i)} - \\sigma(z^{(i)}))^{2}$\n",
    "- The loss function becomes differentiable and convex allowing us to use the powerful $\\textbf{gradient descent}$ optimization algorithm\n",
    "    - Step in the opposite direction of the gradient\n",
    "    - $\\Delta \\mathbf{w} = -\\eta \\nabla_{w} L(\\textbf{w}, b) = \\eta \\frac{2}{n} \\sum\\limits_{i} ( y^{(i)} - \\sigma(z^{(i)}))x_{j}^{(i)}$ \n",
    "    - $\\Delta b = -\\eta \\nabla_{b} L(\\textbf{w}, b) = \\eta \\frac{2}{n} \\sum\\limits_{i} ( y^{(i)} - \\sigma(z^{(i)}))$\n",
    " \n",
    "<img src=\"GradientDescent.png\" width=\"40%\"/>\n",
    "\n",
    "- Gradient descent converges much more quickly if we implement feature scaling\n",
    "- $\\textbf{Standardization}$:\n",
    "    - Shifts the mean of each feature so that it is centered at zero and each feature has a standard deviation of 1 (unit variance)\n",
    "    - $x^{'}_{j} = \\frac{x_{j} - \\mu_{j}}{\\sigma_{j}}$\n",
    "    - where, $x_{j}$ is a vector consisting of the jth feature values of all training examples\n",
    "- $\\textbf{Stochastic Gradient Descent}$:\n",
    "    - Instead of calculating the gradient from the entire data set we randomly pick one data point\n",
    "    - Updates gradient much more frequently and although it is an approximation of the actual gradient it converges much faster\n",
    "    - Fixed learning rate is replace by an adaptive learning rate that decreases over time: $\\frac{c_{1}}{[number \\ of \\ iterations] + c_{2}}$\n",
    "    - $\\textbf{Online Learning}$ - model is trained on the fly as new training data arrives and data can be discarded after updating the model if storage space is an issue\n",
    "- $\\textbf{Mini-batch Gradient Descent}$:\n",
    "    - Middle ground of SGD and full-batch gradient descent\n",
    "    - Apply full-batch to a subset of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3475519-bb69-45f9-80f4-3fb2d31ef825",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Chapter 3: Machine Learning Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819b93d7-05c3-4b4c-8ac0-05c6f5f0833c",
   "metadata": {},
   "source": [
    "### <u> 3.1 Classifiers </u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ec745e-f98b-4ef8-8a04-3eb24ac8d42c",
   "metadata": {},
   "source": [
    "- No free lunch theorem - no single classifier works best across all possible scenarios\n",
    "- It is recommended that you compare the performance of at least a handful of different learning algorithms to select the best model for the particular problem. These may differ in:\n",
    "    - The number of features or examples\n",
    "    - The amount of noise in the dataset\n",
    "    - Whether the classes are linearly separable\n",
    " \n",
    "- The five main steps involved in training a supervised machine learning algorithm:\n",
    "    1. Selecting features and collecting labeled training examples\n",
    "    2. Choosing a performance metric\n",
    "    3. Choosing a learning algorithm and training a model\n",
    "    4. Evaluating the performance of the model\n",
    "    5. Changing the settings of the algorithm and tuning the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e375ee61-4680-4739-9700-314838285165",
   "metadata": {},
   "source": [
    "### <u> 3.2 Overfitting </u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba27d5e-97fc-4297-86bb-7431f22db2f3",
   "metadata": {},
   "source": [
    "- $\\textbf{Overfitting}$ - a model performs well on training data but does not generalize well to unseen data (test data)\n",
    "- It is said the model suffers from having high variance which can be caused by having too many parameters, leading to a model that is too complex for the underlying data\n",
    "- $\\textbf{Underfitting}$ - (high bias) the model is not complex enough to capture the pattern in the training data well and suffers from low performance on unseen data\n",
    "\n",
    "<img src=\"Overfitting.png\" width=\"50%\"/>\n",
    "\n",
    "- $\\textbf{bias-variance tradeoff}$:\n",
    "    - high variance refers to overfitting\n",
    "    - high bias refers to underfitting\n",
    " \n",
    "- One way of finding a good bias-variance tradeoff is to tune the complexity of the model via regularization\n",
    "- $\\textbf{Regularization}$ - introduce additional information to penalize extreme parameter (weight) values. Useful method for handling:\n",
    "    - colinearity - high correlation among features\n",
    "    - filtering out noise from the data\n",
    "    - preventing overfitting\n",
    "- $\\textbf{L2 regularization}$ - (L2 shrinkage / weight decay) most common form of regularization. Adds the following term to the loss function:\n",
    "    - $\\frac{\\lambda}{2n} ||\\textbf{w} ||^{2} = \\frac{\\lambda}{2n} \\sum\\limits_{j=1}^{m} w_{j}^{2}$\n",
    "    - Regularization parameter, $\\lambda$ - used to control how closely we fit the traing data, while keeping the weights small\n",
    "    - Parameter C implemented in Logistic Regression class in scikit-learn comes from SVM and is inversely proportional to the $\\lambda$\n",
    "    - If the regularization strength is set too high the weight coefficients approach zero and the model can perform poorly due to underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edf4e2b-ef22-4186-b986-9943330a877b",
   "metadata": {},
   "source": [
    "### <u> 3.3 Logistic Regression </u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e478bcfd-d36f-4b40-9a91-a3bbd5964a43",
   "metadata": {},
   "source": [
    "- Linear model used for binary classification\n",
    "- Very easy to implement and performs very well on linearly separable classes\n",
    "- Easily updated, which is attractive when working with streaming data\n",
    "- The activation function becomes the sigmoid function which maps any number back to [0, 1] range\n",
    "- The output of the sigmoid function is then interpreted as the probability of a particular example belonging to a class\n",
    "- This probability is often just as interesting as the predicted class label\n",
    "- It is recommended to use more advanced approaches than regular SGD (newton-cg, lbfgs, liblinear, sag, saga)\n",
    "\n",
    "<img src=\"LogisticRegression.png\" width=\"50%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b055e70-7746-48f5-85d8-18375097d75c",
   "metadata": {},
   "source": [
    "### <u> 3.4 Support Vector Machines (SVM) </u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0dd6d6f-c9e3-4383-b87f-04cc50c1efdc",
   "metadata": {},
   "source": [
    "- Extension of the perceptron\n",
    "- Optimization objective is to maximize the margin\n",
    "- Less prone to outliers than logistic regression, but is more complex to implement and derive\n",
    "\n",
    "- $\\textbf{Margin}$ - the distance between the separating hyperplane (decision boundary) and the training examples that are closest to this hyperplane\n",
    "- $\\textbf{Suppor Vectors}$ - training examples that are closest to the decision boundary\n",
    "- The rationale is decision boundaries with large margins tend to have a lower generalization error, whereas models with small margins are more prone to overfitting\n",
    "\n",
    "<img src=\"SVM.png\" width=\"50%\"/>\n",
    "\n",
    "- The slack variable was introduced because the linear contraints in the SVM optimization in the presence of misclassifications, under appropriate loss penalization. This slack variable introduces the variable C in SVM context.\n",
    "- $\\textbf{C}$ - hyperparameter for controlling the penalty for misclassification. (smaller C less strict misclassification errors) Can be used to control the width of the margin and therefore tune the bias-variance tradeoff\n",
    "\n",
    "<img src=\"CHyperparameter.png\" width=\"50%\"/>\n",
    "\n",
    "- SVM can be easily kernalized to solve nonlinear classification problems\n",
    "- kernel methods deal with linearly inseparable data by creating a nonlinear combination of the original features to project them onto a higher-dimnsional space via a mapping function, $\\phi$\n",
    "- Solving a nonlinear problem using SVM:\n",
    "    1. Transform the training data into a higher-dimensional feature space using the mapping function, $\\phi$\n",
    "    2. Train a linear SVM model to classify the data in the new feature space\n",
    "    3. Use $\\phi$ to transform new, unseen data to classify it using the linear SVM model\n",
    "\n",
    "<img src=\"KernalSVM.png\" width=\"50%\"/>\n",
    "\n",
    "- Contruction of the new features is computationally very expensive, especially with high-dimensional data\n",
    "- $\\textbf{Kernal trick}$ - define a $\\textbf{kernal function}$, $\\phi(x^{(i)})^{T}\\phi(x^{(j)})$, (usually radial basis function (RBF) kernal or the Gaussian kernal) to save the expensive step of calculating the dot product $x^{(i)T}x^{(i)}$\n",
    "- 'Kernel' can be interpreted as a similarity function between a pair of examples\n",
    "- In Gaussian kernel we have the hyperparameter $\\gamma = \\frac{1}{2\\sigma^{2}}$ which can be understood as a cut-off parameter for the Gaussian sphere. Higher $\\gamma$ increases the influence of the training examples which leads to a tighter and bumpier decision boundary\n",
    "- $\\gamma$ plays an important role in controlling overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8dc999-ebcd-450e-b719-66dddae6572e",
   "metadata": {},
   "source": [
    "### <u> 3.5 Decision Tree </u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ec0f69a-8897-460f-858e-c24f4354096c",
   "metadata": {},
   "source": [
    "<img src=\"DecisionTree.png\" width=\"30%\"/>\n",
    "\n",
    "- Attractive models if we care about interpretability\n",
    "- Breaking down the data by making a decision based on asking a series of questions\n",
    "- The decision tree model learns a series of questions to infer the class labels of the examples\n",
    "- Start at the tree root and split the data on the feature that results in the largest information gain. We continue this splitting until the leaves are pure.\n",
    "- Can easily lead to overfitting because of how deep the trees can become. Therefore, we want to prune the tree by setting the mimit for the maximum depth of the tree\n",
    "- Our objective funciton is defined to maximize the IG at each split:\n",
    "    - $IG(D_{p},f) = I(D_{p}) - \\sum\\limits_{j=1}^{m} \\frac{N_{j}}{N_{p}}I(D_{j})$\n",
    "    - f is the feature to perform the split\n",
    "    - $D_{p}$ and $D_{j}$ are the dataset of the parent and the jth child node\n",
    "    - I is the impurity measure\n",
    "    - $N_{p}$ is the total number of training examples at the parent node\n",
    "    - $N_{j}$ is the number of examples in the jth child node\n",
    "    - The information gain is the difference between the impurity of the parent node and the sum of the child node impurities.\n",
    "- Most libraries implement binary decision trees where the parent node is split into two child nodes $D_{left}$ and $D_{right}$ to reduce the combinatorial search space\n",
    "    - $IG(D_{p},f) = I(D_{p}) - \\frac{N_{left}}{N_{p}}I(D_{left}) - \\frac{N_{right}}{N_{p}}I(D_{right})$\n",
    "- Impurity Measures or Splitting criteria:\n",
    "    - Gini impurity ($I_{G}$)\n",
    "        - $I_{G}(t) = \\sum\\limits_{i=1}^{c} p(i|t) (1 - p(i|t)) = 1 - \\sum\\limits_{i=1}^{c}p(i|t)^{2}$\n",
    "    - Entropy ($I_{H}$)\n",
    "        - $I_{G}(t) = - \\sum\\limits_{i=1}^{c} p(i|t)\\log_{2}p(i|t)$\n",
    "        - $p(i|t)$ is the proportiion of examples that belong to class i for a particular node, t\n",
    "    - Classification Error ($I_{E}$)\n",
    "        - $I_{E}(t) = 1 - \\max[p(i|t)]$\n",
    "        - Useful for pruning,but not recommended for growing a decision tree, since it is less sensitive to changes in the class probabilities of the nodes\n",
    "\n",
    "<img src=\"DTImpurities.png\" width=\"40%\"/>\n",
    "\n",
    "- Gini impurity and entropy typically yield very similar results, and it is often not worth spending much time on evaluating trees using different impourity criteria rather than experimenting with different pruning cut-offs\n",
    "\n",
    "- Decision trees can build complex decision boundaries by dividing the feature space into rectangles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8d2c3c-eb59-4517-9cae-9a9bbee4069c",
   "metadata": {},
   "source": [
    "### <u> 3.6 Random Forest </u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407e7e9d-f447-4e59-a79b-2d1b0c476615",
   "metadata": {},
   "source": [
    "- Good scalability and easy to use\n",
    "- Not as interpretable as decision trees but you do not have to worry about choosing good hyperparameters and typically does not need to be pruned\n",
    "- Ensemble of decision trees\n",
    "- Average multiple (deep) decision trees that individually suffer from high variance to build a more robust modl that has a better generalization performance and is less susceptible to overfitting\n",
    "- Random forest algorithm:\n",
    "    1. Draw a random bootstrap sample of size n(randomly choose n examples from the training dataset with replacement)\n",
    "    2. Grow a decision tree from the bootstrap sample. (note: instead of evaluating all features to determine the best split at each node, we only consider a random subset of these.) At each node:\n",
    "        - Randomly select d feature without replacement\n",
    "        - Split the node using the feautre that provides the best split according to the objective function, for instance, maximizing the information gain\n",
    "    3. Repeat steps 1-2 times\n",
    "    4. Aggregate the prediction by each tree to assign the class label by majority vote. \n",
    "- Hyperparameters:\n",
    "    - k - number of trees: the larger the number of trees, the better the performance of the random forest classifier at the expense of increased computational cost\n",
    "    - n - the size of the bootstrap sample: decreasing the boostrap sample increases diversity among the individual trees (increased randomness) but smaller samples typically result in a lower overall performance and small gap between training and test performance. Too larger and you increase the degree of overfitting\n",
    "    - d - number of features that are randomly chosen for each split: Choose a value smaller than the total number of features in the training dataset, m. Often chosen as $d = \\sqrt{m}$\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568e2f45-6501-41d4-ba7b-47515c4487bd",
   "metadata": {},
   "source": [
    "### <u> 3.7 K-nearest neighbors (KNN) </u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98214138-b358-4c10-af08-e59dedb3b009",
   "metadata": {},
   "source": [
    "- $\\textbf{Lazy Learner}$ - does not learn a discriminative function from the training data, but memorizes the training dataset instead\n",
    "- KNN Algorithm:\n",
    "    1. Choose the number of k and a distance metric\n",
    "    2. Find the k-nearest neighbors of the data record that we want to classify\n",
    "    3. Assign the class label by majority vote\n",
    " \n",
    "<img src=\"KNN.png\" width=\"30%\"/>\n",
    "\n",
    "- choosing k and an appropriate distance cutoff is crucial for bias-variance tradeoff (Eucludean distance if data is standardized)\n",
    "- Very susceptible to overfitting due to the curse of dimensionality (data becomes increasly sparse with more dimensions)\n",
    "- Can you use feature selection and dimension reduction techniques to help avoid this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ba6d74-99b2-42ac-9d12-c4130b550afe",
   "metadata": {},
   "source": [
    "### <u> 3.8 Summary </u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8768a1b-6ede-4e17-ae88-d34b57a4a833",
   "metadata": {},
   "source": [
    "- Decision Trees are particularly attractive if we care about interpretability\n",
    "- Logisitic Regression is useful for online learning via SGD and allows us to predict the probability of a particular event\n",
    "- SVMs are powerful linear models that can be extended to non-linear problems via the kernel trick, but have many parameters that have to be tuned\n",
    "- Random Forests - don't require much parameter tuning and don't overfit as easily as decision trees\n",
    "- KNN is an alternative approach to classification via lazy learning that allows us to make predictions without any model training, but with a more computationally expensive prediction step\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f75acd2-a913-4d07-8772-64f02c33ec9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "perceptron = SGDClassifier(loss='perceptron')\n",
    "logisticRegression = SGDClassifier(loss='log')\n",
    "svm = SGDClassifier(loss='hinge')\n",
    "tree_model = tree.DecisionTreeClassifier(criterion='gin1', max_depth=4, random_state=1)\n",
    "forest = RandomForestClassifier(n_estimators=25, random_state=1, n_jobs=2)\n",
    "knn = KNeighborsClassifier(n_neighbors=5, p=2, metric='minkowski')\n",
    "\n",
    "#tree_model.fit(X_train, y_train)\n",
    "#tree.plot_tree(tree_model, feature_names = feature_names, filled=True)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad1d8b0-9aaa-4ecf-8a5f-b29e747a30a5",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Chapter 4: Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85246159-99b8-44f3-8b89-5a4f4ab4f704",
   "metadata": {},
   "source": [
    "### <u> 4.1 Missing Data </u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff839e53-1d1d-436d-8f35-11e1e92331a6",
   "metadata": {},
   "source": [
    "- In real-world applications it is common for training examples to be missing one or more values (blank, NaN, NULL)\n",
    "- Can use isnull() in Pandas to identify the number of missing values in a data frame\n",
    "    - df.isnull().sum() \n",
    "1. $\\textbf{Eliminate Training examples or features with missing values}$\n",
    "    - Drop training examples with missing values\n",
    "        - df.dropna(axis=0)\n",
    "    - Drop rolumns that have at lease one NaN\n",
    "        - df.dropna(axis=1)\n",
    "    - Drop rows where all values NaN\n",
    "        - df.dropna(how='all')\n",
    "    - Drop rows that have fewer than x values\n",
    "        - df.dropna(thresh=x)\n",
    "    - Drop rows where NaN in columns X,Y,...\n",
    "        - df.dropna(subset=['X','Y'])\n",
    "2. $\\textbf{Input missing values}$\n",
    "    - Dropping data may not be feasible because we might lose too much valuable data\n",
    "    - Instead we use interpolation techniques to estimate the missing value from the other trainging examples\n",
    "        - $\\textbf{Mean Imputation}$ - replace the missing value with the mean values of the entire feature column\n",
    "            - sklearn.impute SimpleImputer \n",
    "            - df.fillna(df.mean())\n",
    "            - KNNImputer\n",
    "- Scikit-learn Transformer API:\n",
    "    - fit - learn the parameters from the training data\n",
    "    - transform - use fit parameters to transform the data\n",
    "    - Data array needs to have the same number of features as the data array that was used to fit the model\n",
    "\n",
    "<img src=\"TransformerAPI.png\" width=\"40%\"/>\n",
    "\n",
    "- Scikit-learn Estimators API:\n",
    "    - Have a predict method, but also have a tranform method\n",
    "    - fit - learn the parameters of a model (such as used in the estimators for classification)\n",
    "    - In supervised learning tasks, we also provide the class labels for the fitting the model, which can theb be used to make predictions about new, unlabeled data examples using predict\n",
    " \n",
    "<img src=\"EstimatorsAPI.png\" width=\"30%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320b18c6-d8dc-4b81-a088-8d316bb5064c",
   "metadata": {},
   "source": [
    "### <u> 4.2 Categorical Data </u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b3f923-d567-4ae6-91b3-e1ec487ceb3f",
   "metadata": {},
   "source": [
    "- Non-numerical data:\n",
    "    - $\\textbf{Ordinal}$ - categorical values that can be sorted or ordered (t-shirt sizes)\n",
    "    - $\\textbf{Nominal}$ - don't imply any order (t-shirt colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5dcace-bd86-4456-b137-8827714d0f88",
   "metadata": {},
   "source": [
    "#### 4.2.1 Mapping Ordinal Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2770e96-351b-4e6f-84b8-bd7e69b3f2a0",
   "metadata": {},
   "source": [
    "- Learning algorithms do not use ordinal inforrrmation inc class labels\n",
    "- We need to map the catergorical string values into integers\n",
    "    - mapping must be defined manually\n",
    "        - mapping = {'M':1, 'L':2, 'XL':3}\n",
    "        - reverse_mapping = { v: k for k, v in size_mapping.items()}\n",
    "    - Pandas map method can be use with defined mapping\n",
    "        - df['size'].map(mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0824e67-31a9-4b21-95fe-327619cc7ada",
   "metadata": {},
   "source": [
    "#### 4.2.2 Encoding Class Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44c069e-6715-4ada-a862-64f19d6c629e",
   "metadata": {},
   "source": [
    "- Some machine learning libraries require that class labels are encoded as integer values\n",
    "- Most estimators in scikit-learn convert class labels to integers internally\n",
    "- Map to integers similar to the ordinal features\n",
    "- Also there is a LabelEncoder in sklearn.preprocessing that also does this\n",
    "    - fit_transform\n",
    "    - inverse_transform "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4537b575-e045-402b-be21-a347e0e45f90",
   "metadata": {},
   "source": [
    "#### 4.2.3 One-Hot Encoding on Nominal Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64eec3e-57a3-48f6-835b-b5053b227a4c",
   "metadata": {},
   "source": [
    "- Since order does not matter we can use the LabelEncoder to encode string labels into integers\n",
    "- Most common mistakes in dealing with nominal categorical data: The data is nominal, but the integers say that one category is larger than the other. You would still obtain useful results, but not optimal results. The solution to this is One-Hot encoding.\n",
    "- $\\textbf{One-hot encoding}$ - create a new dummy feature for each unique value in the nominal feature column that is binary.\n",
    "    - sklearn.preprocessing has a OneHotEncoder API\n",
    "    - sklearn.compise has ColumnTransformer to apply to select columns\n",
    "    - get_dummies(df, drop_first=True) method from pandas - only effects string column\n",
    "- Introduces multicolinearity which can be an issue for certain methods (methods that require matrix inversion)\n",
    "- Can remove one feature column from the one-hot encoded array to reduce correlation among variables\n",
    "- If categorical features have high caridnality then we may need to use alternative methods to One-Hot Encoding\n",
    "    - $\\textbf{Binary Encoding}$ - produces multiple binary features, but requires fewer columns. Numbers are converted into binary and then each binary number position will form a new feature column\n",
    "    - $\\textbf{Count Encoding}$ or $\\textbf{Frequency Encoding}$ - replaces the label of each category by the number of times or frequency it occurs in the data set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e01fb0-8f2c-4827-80bc-c3b8575f5e9b",
   "metadata": {},
   "source": [
    "#### 4.2.4 Encoding Ordinal Features (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d54fc0b-aaee-45e0-ae9a-281ac4b61aad",
   "metadata": {},
   "source": [
    "- If you are unsure about the numerical differences between categories of ordinal features, or that difference is not defined, we can also encode them using a threshold encoding with binary values\n",
    "- T-Shirt sizes of M, L, and XL can be divided into 2 columns of x > M and X > L. (relative differences are not accounted for)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5fd17be-a33e-4532-9593-c44d5bed8bf4",
   "metadata": {},
   "source": [
    "### <u> 4.3 Partitioning Data into Training and Test Datasets </u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288ee966-8656-4251-9431-ed2b6bea6ca1",
   "metadata": {},
   "source": [
    "- We divide our data into serparate datasets for training and testing\n",
    "- Comparing predicitions to true labels in the test set can be understood as the unbiased performance evaluation of our model before applying in the real-world\n",
    "- Stratify assures the same proportions of each class to the original dataset\n",
    "- Choosing the ratio:\n",
    "    - The larger the test set, the more we are withholding valuable information\n",
    "    - The smaller the test set, the more inaccurate the estimation of the generalization error\n",
    "    - Most common splits are 60:40, 70:30, 80:20 depending on the size of the initial dataset\n",
    "    - 90:10 or 99:1 for very large datasets (>100k)\n",
    "- Instead of discarding the allocated test data after the model training and evaluation it is common practive to retrain a classifier on the entire dataset, as it can improve the predictive performance of the model. (Generally recommended, but if the training data set is small and the test data contains outliers then this could lead to worse generalization performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8456e32-0a3a-4938-a511-8cfce68a2480",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c091ebc-3e7f-4171-b943-063c59b104b6",
   "metadata": {},
   "source": [
    "### <u> 4.4 Bringing Features onto the Same Scale </u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b980a1cb-0924-43b3-8486-26ed7946fd63",
   "metadata": {},
   "source": [
    "- $\\textbf{Feature Scaling}$ is a crucial step in data preprocessing\n",
    "- Decision trees and random forests are 2 of the very few machine learning algorithms where we don't need to worry about feature scaling. (scale-invariant)\n",
    "- Most machine learning optimaization algorithms behave much better iff the features are on the same scale\n",
    "- Two most common ways of scaling features are normalization and standardization\n",
    "- $\\textbf{Normalization}$ - rescaling of the features to a range of [0, 1] (special case of min-max scaling)\n",
    "    - $x^{(i)}_{norm} = \\frac{x^{(i)} - x_{min}}{x_{max} - x_{min}}$\n",
    "    - useful when we need values in a bounded interval\n",
    "- $\\textbf{Standardization}$ - Shifts the mean of each feature so that it is centered at zero and each feature has a standard deviation of 1 (unit variance)\n",
    "    - $x^{(i)}_{std} = \\frac{x^{(i)} - \\mu_{x}}{\\sigma_{x}}$\n",
    "    - does not change the shape of the distribution and does not convert non-normally distributed data into normally distributed data\n",
    "    - maintains useful information about outliers and makes the algorithm less sensitive to them unlike normalization which scales the data to a limited range\n",
    "- $\\textbf{Robust Scaling}$ - scales the data based on the 25th and 75th quantile, so outliers become less pronounced\n",
    "    - Small datasets that contain many outliers\n",
    "    - If the machine learnign algorithm applied to this dataset is prone to overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f256b539-c380-414e-9a59-7fbfcb8d0c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fromsklearn.preprocessing import StandardScaler\n",
    "stdsc = StandardScaler()\n",
    "X_train_std = stdsc.fit_transform(X_train)\n",
    "X_test_std = stdsc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d944e0-168a-4f2e-af7b-bd4279ff52fe",
   "metadata": {},
   "source": [
    "### <u> 4.5 Selecting Meaningful Features </u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf08f275-2317-41ef-9bae-85595813bda1",
   "metadata": {},
   "source": [
    "- If a model performs much better on a training dataset than on the test data set, this observation is a strong indicator of overfitting.\n",
    "- Common solutions to reduce generalization error:\n",
    "    - Collect more training data\n",
    "    - Introduce a penalty for complexity via regularization\n",
    "    - Choose a simpler model with fewer parameters\n",
    "    - Reduce the dimensionality of the data\n",
    " \n",
    "- Sometime more training data is often not applicable therefore we look to common ways to reduce overfitting\n",
    "    - regularization\n",
    "    - dimensionality reduction via feature selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6765b01d-2181-40eb-8a22-908845909d50",
   "metadata": {},
   "source": [
    "#### 4.5.2 L2 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0f6369-7c6f-4c04-8e46-892a473bb1f6",
   "metadata": {},
   "source": [
    "- Reduce the complexity of a model by penalizing large individual weights by adding a penalty term to the loss function\n",
    "- L2 norm of the weight vector\n",
    "    - $||\\textbf{w}||^{2}_{2} = \\sum\\limits_{j=1}^{m} w^{2}_{j}$\n",
    "- Regularization parameter, $\\lambda$\n",
    "\n",
    "##### Geometric Interpretation\n",
    "<img src=\"L2Reg.png\" width=\"30%\"/>\n",
    "\n",
    "- L2 regularization term is represented by the shaded ball\n",
    "- Our weight coefficients cannot fall outside the shaded area\n",
    "- Under the penalty contraint, our best effort is to choose the point where the L2 ball intersects with the contours of the unpenalized loss function\n",
    "- The larger the value of $\\lambda$ the faster the penalized loss grows, which leads to a narrower L2 ball\n",
    "- Our goal is to minimize the sum of the unpenalized loss plus the penalty term, This can be understood as adding bias and perferring a simpler model to reduce the variance in the absence of sufficient training data to fit the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd7714d-9042-48a6-9d1a-1d8fe9a3bc78",
   "metadata": {},
   "source": [
    "#### 4.5.1 L1 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b573e4-1029-4605-967c-bd2fee36be22",
   "metadata": {},
   "source": [
    "- Reduce the complexity of a model\n",
    "- L1 norm of the weight vector\n",
    "    - $||\\textbf{w}||_{i} = \\sum\\limits_{j=1}^{m} |w_{j}|$\n",
    "- Usually yields sparse feature vectos, and most feature weights will be zero.\n",
    "- Sparsity can be useful if we have high-dimensional dataset with many features that are irrelevant (especially if we have more irrelevant dimensions than training examples)\n",
    "- set penalty to 'l1' in scikit-learn mdeols that support it\n",
    "\n",
    "##### Geometric Interpretation\n",
    "\n",
    "<img src=\"L1Reg.png\" width=\"30%\"/>\n",
    "\n",
    "- Similar to L2 except L1 is the sum of the absolute weight conditions we represent it as a diamond-shape budget\n",
    "- We can see the contour of the loss function touches the L1 diamond at $w_{1} = 0$\n",
    "- Since the contours of an L1 regularized system are sharp, it is more likely that the optimum (intersection between the ellipses of the loss function and the boundary of the L1 diamond) is located on the axes, which encourages sparsity\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61472a9b-e945-481b-ad6c-04302d88e647",
   "metadata": {},
   "source": [
    "#### 4.5.3 Sequential Feature Selection Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbd36150-e49e-4c75-945b-f7a95a83b545",
   "metadata": {},
   "source": [
    "- An alternative way to reduce the complexity of the model and avoid overfitting is dimensionality reduction via feature selection, especially useful for unregularized models\n",
    "- 2 categories of dimensionality reduction techniques\n",
    "    - $\\textbf{Feature Selection}$ - select a subset of the original feature\n",
    "    - $\\textbf{Feature Extraction}$ - derive information from the feature set to construct a new feature subspace\n",
    "- Sequential feature selection algorithms are a family of greedy search algorithms that are used to reduce an intial d-dimensional feature space to a k-dimensional feature subspace where k<d\n",
    "- The motivation is to automatically select a subset of features that are most relevant to the problem, to improve computational efficiency, or to reduce the generalization eror of the model by removing irrelevant features or noise (good for algorithms that don't support regularization)\n",
    "- $\\textbf{Sequential Backward Selection (SBS)}$ - aims to reduce dimensionality of the intial feature subspace with a minimum decay in the performance of the classifier to improve upon computational efficiency. Can improve predictive power of a model if it suffers from overfitting\n",
    "- The idea behind SBS algorithm is to sequentially remove features from the full feature subset until the new feature subspace contains the desired number of features \n",
    "- To choose which feature to remove at each stage, we define a criterion function, J, that we want to minimize\n",
    "- We eliminate the feature that causes the least performance loss after removal\n",
    "- SBS Algorithm:\n",
    "    1. Initialize the algorithm with k=d, where d is the dimensionality of the full feature space, $X_{d}$\n",
    "    2. Determine the feature, $x^{`}$, that maximizes the criterion: $X^{`} = argmax J(X_{k} - x)$, where $x \\ \\epsilon \\ X_{k}$.\n",
    "    3. Remove the feature, $x^{`}$, from the feature set: $X_{k-1} = X_{k} - x^{`}$; $k = k-1$\n",
    "    4. Terminate if $k$ equals the number of desired features, otherwise, go to step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85834b5-6e08-4808-a5e6-4e68392dd4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "X, y = load_iris(return_X_y=True)\n",
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "sfs = SequentialFeatureSelector(knn, n_features_to_select=3)\n",
    "sfs.fit(X, y)\n",
    "sfs.get_support()\n",
    "sfs.transform(X).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9035723-884e-4805-b252-52077090ffdd",
   "metadata": {},
   "source": [
    "#### 4.5.4 Assessing Feature importance with random forests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc564e64-df21-45b0-b27c-9adc13c5a32a",
   "metadata": {},
   "source": [
    "- Using random forest, we can measure the feature importance as the averaged impurity decrease computed from all decision trees in the forest\n",
    "- Scikit-Learn collexts the feature importance values for us in feature_importances_ attribute after fitting a RandomForestClassifier.\n",
    "- Feature importance will sum up to 1\n",
    "- If two more more features are highly correlated, one feature may be ranked vary highly while the information on the other feature(s) may not be fully captured - we do not need to be concerned with this if we are merely interested in the predictive performance of a model ranther than the interpretation of feature importance values\n",
    "  \n",
    "<img src=\"RandomForestFS.png\" width=\"30%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5667750c-a98d-417a-896f-d62f35930087",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Chapter 5: Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bb05663-df6d-45ae-ba65-15ff65fe2c9c",
   "metadata": {},
   "source": [
    "### 5.1 Feature Extraction With Principal Component Analysis (PCA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce97f33-d4ac-4793-acbd-3abc19404813",
   "metadata": {},
   "source": [
    "- We can use feature extraction techniques to reduce the number of features in a dataset by transforming or projecting the data onto a new feature space\n",
    "- $\\textbf{Feature extraction}$ - an approach to data compression with the goal of maintaining most of the relevant information\n",
    "    - improve storage space\n",
    "    - improve compuutation efficiency of the learning algorthm\n",
    "    - improve predictive performance by reducing the curse of dimensionality (especially with non-regularized models)\n",
    "\n",
    "#### PCA\n",
    "- Unsupervised linear transformation technique used for feature extraction and dimensionality reduction\n",
    "- Other popular applications of PCA:\n",
    "    - Exploratory data analysis\n",
    "    - Denoising of signals in stock market trading\n",
    "    - Analysis of genome data and gene expression levels in the filed of bioinformatics\n",
    "- Helps to identify patterns in data basedo nt he correlation between features\n",
    "- Aims to find the directions of maximum vairance in high-dimensional data and projects the data onto a new subspace with equal or fewer dimensions than the original one.\n",
    "- The orthogonal axes (principal compnents) of the new subspace are the directions of maximum variance given the constraint that the new feature axes are orthogonal to each other\n",
    "\n",
    "<img src=\"PCA.png\" width=\"40%\"/>\n",
    "\n",
    "- Given that we transform the original $d$-dimensional data onto a new $k$-dimensional subspace (typically $k$ << $d$), the first principal compoenet will have the largest possible variance.\n",
    "- Each succesive principal component will have the largest variance given the contraint that it is orthogonal to the other principal components\n",
    "- Highly sensitive to data scaling (features need to be standardized)\n",
    "\n",
    "- PCA Algorithm:\n",
    "    1. Standardize the $d$-dimensional dataset\n",
    "    2. Construct the covariance matrix\n",
    "        - $\\Sigma = \\begin{bmatrix} \\sigma_{1}^{2} & \\sigma_{12} & \\sigma_{13} \\\\ \\sigma_{21} & \\sigma_{2}^{2} & \\sigma_{23} \\\\ \\sigma_{31} & \\sigma_{32} & \\sigma_{3}^{2} \\end{bmatrix}$\n",
    "        - $ \\sigma_{jk} = \\frac{1}{n - 1} \\sum\\limits_{i=1}^{n} (x_{j}^{(i)} - \\mu_{j})(x_{k}^{(i)} - \\mu_{k})$\n",
    "    4. Decompose the covariance matrix into its eigenvectors and eigenvalues\n",
    "        - $\\Sigma \\textbf{v} = \\lambda \\textbf{v}$\n",
    "        - $\\lambda$ - eigenvalue\n",
    "    6. Sort the eigenvalues by decreasing order to rank the corresponding eigenvectors\n",
    "    7. Select $k$ eigenvectors, which correspond to the $k$ largest eigenvalues, where $k$ is the dimensionality of the new feature ssubspace ($k \\leq d$)\n",
    "    8. Construct a projection matrix, $W$, from the 'top' $k$ eignevectors\n",
    "    9. Transform the $d$-dimensional input dataset, $X$, using the projection matrix, $W$, to obtain the new $k$-dimensional feature subspace\n",
    "        - Transform a training example - $x^{`} = xW$\n",
    "        - Transform the whole data set - $X^{`} = XW$\n",
    "##### Total and Explained Variance\n",
    "- Select a subset of eigenvectors that contains most of the information (variance)\n",
    "- $\\textbf{Variance Explained Ratios}$ of eigenvalue, $\\lambda_{j}$ - how much of the variance does the eigenvector account for:\n",
    "    - Explained variance ratio $= \\frac{\\lambda_{j}}{\\sum\\limits_{j=1}^{d} \\lambda_{j}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5907d9e7-f300-4c48-b051-ceb88fbf9f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "X_train_pca = pca.fit_transform(X_train_std)\n",
    "X_test_pca = pca.transform(X_test_std)\n",
    "\n",
    "pca = PCA(n_components=None)\n",
    "pca.explained_variance_ratio_\n",
    "\n",
    "# Plot feature contributions for PC1\n",
    "sklearn_loadings = pca.components_.T * nps.sqrt(pca.explained_variance_)\n",
    "fig, ax = plt.sunplots()\n",
    "ax.bar(range(nFeatures), sklearn_loadings[:, 0], align='center')\n",
    "plt.ylim([-1, 1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002048b5-6cdf-4fb1-9f73-1b915a765b5b",
   "metadata": {},
   "source": [
    "### 5.2 Linear Discriminant Analysis (LDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5422ac0-6602-47ba-aeb0-bff29d90b9cf",
   "metadata": {},
   "source": [
    "- Technique for feature extraction to increase computational efficiency and reduce the degree of overfitting due to the curse of dimensionality in non-regularized models\n",
    "- Unlike PCA which attempts to find the orthogonal component axes of maximum variance in a dataset, the goal of LDA is to find the feature subspace that optimizes class seperability\n",
    "- Assumes:\n",
    "    - Data is normally distributed\n",
    "    - Classes have identical covariance matrices\n",
    "    - The training exampels are statistically independent of each other\n",
    "- Can still work if one, or more, assumptions is (slightly) violated\n",
    "- At most there are $c - 1$ linear discriminants, where $c$ is the number of class labels\n",
    "- LDA Algorithm:\n",
    "    1. Standardize the $d$-dimensional dataset\n",
    "    2. For each class, compute the $d$-dimensional mean vector\n",
    "        - $\\textbf{m}_{i} = \\frac{1}{n} \\sum\\limits_{x \\epsilon D_{i}} \\textbf{x}_{m}$\n",
    "        - Each mean vector, $\\textbf{m}_{i}$, stores the mean feature value, $\\mu_{m}$\n",
    "    5. Construct the between-class scatter matrix, $S_{B}$, and the within-class scatter matrix, $S_{w}$\n",
    "       - $ \\textbf{S}_{w} = \\sum\\limits_{i=1}^{c} \\textbf{S}_{i} $\n",
    "       - $\\textbf{S}_{i} = \\sum\\limits_{x \\epsilon D_{i}} (x - \\textbf{m}_{i})(x - \\textbf{m}_{i})^{T}$\n",
    "       - $\\textbf{S}_{B} = \\sum\\limits_{i=1}^{c} n_{i} (\\textbf{m}_{i} - \\textbf{m}) (\\textbf{m}_{i} - \\textbf{m})^{T}$\n",
    "       - $\\textbf{S}_{i}$ - scatter matrix of class $i$\n",
    "       - $\\textbf{m}$ - overall mean computed from all classes\n",
    "    7. Compute the eigenvectors and corresponding eigenvlaues of the matrix, $S_{w}^{-1}S_{B}$\n",
    "    9. Sort the eigenvalues by decreasing order to rank the corresponding eigenvectors\n",
    "    10. Choose the $k$ eigenvectors that correspond to the $k$ larges eigenvalues to construct a $d\\times k$-dimensional transformation matrix, $W$; the eigenvectors are the columns of this matrix\n",
    "    11. Project the examples onto the new feature subspace using the transformation matrix, $W$. \n",
    "\n",
    "<img src=\"LDA.png\" width=\"40%\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa7dd64-c933-4e1d-a36a-14484d04c418",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LineaqrDiscriminantAnalysis as LDA\n",
    "lda = LDA(n_components=2)\n",
    "X_train_lda = lda.fit_transform(X_train_std)\n",
    "X_test_lda = lda.transform(X_test_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1559875c-9928-4c80-8b13-34df82312f23",
   "metadata": {},
   "source": [
    "### 5.3 Non-Linear Dimensionality Reduction and t-Distributed Stochastic Neighbor Embedding (t-SNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24442506-3b57-4078-a60d-1074f33e191f",
   "metadata": {},
   "source": [
    "- If we are dealing with nonlinear problems, linear transformation techniques for dimensionality reduction (PCA and LDA) may not be the best choice\n",
    "- The development and application of nonlinear dimensionality reduction techniques is often reffered to as manifold learning\n",
    "- Algorthms for manifold learning have to capture the complicated structure of the data in order to project it onto a lower-dimensional space where the relationship between data points is preserved.\n",
    "\n",
    "<img src=\"NonlinearProblem.png\" width=\"40%\"/>\n",
    "\n",
    "- Manifold learning algorithms are very powerful, but are notoriously hard to use, and with non-ideal hyperparameter choices they may cause more harm than good\n",
    "- The reason behind this difficult is that we are often working with high-dimensional datasets that we cannot readily visualize and where the structure is not obvious\n",
    "- Also, if we don't project the dataset into two or three dimensions it is hard or even impossible to assess the quality of the results\n",
    "\n",
    "#### t-SNE\n",
    "\n",
    "- In a nutshell, it is modeling data points based on their pair-wise distances in the high-dimensional feature space\n",
    "- It finds a probability distribution of pair-wise distances in the new, lower dimensional space that is close to the probability distribution of pair-wise distances in the original dataset\n",
    "- t-SNE learns to embed data points into a lower-dimensional space suc that the pairwise distances inthe original space are preserved\n",
    "- It is intended for visualization purposes as it requires the whole dataset for the projection. Since it projects the points directly, we cannot apply t-SNE to new data points\n",
    "- Hyperparameters:\n",
    "    - Perplexity\n",
    "    - Learning Rate (epsilon)\n",
    " \n",
    "#### Uniform manifold approximation and projection (UMAP)\n",
    "- Another popular visualizaiton technique\n",
    "- Can produce similarly good results sd t-SNE\n",
    "- Typically faster than t-SNE\n",
    "- Can be used to project new data, whcih makes it more attractive as a dimensionality reduction technique in a machine learning context\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41853d7-df33-401b-b7fb-aee6016570ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "tsne = TSNE(n_components=2, init='pca', random_state=123)\n",
    "X_digits_tsne = tsne.fit_transform(X_digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288f0b78-0d92-408d-9ae9-b27a7ae93dff",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Chapter 6: Model Evaluation and Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8479b5-0774-48d7-a5b0-28e9f1a03711",
   "metadata": {},
   "source": [
    "### 6.1 Streamlining Workflows with Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2853bf-8395-4860-a946-6caa1af2b90d",
   "metadata": {},
   "source": [
    "- Instead of going through model fitting and data transformation steps for the training and test data sets seprately, we can chosin them into a pipeline\n",
    "- make_pipeline function takes an aritrary number of scikit-learn transformers, followed by a scikit-learn estimator that implements the fit and predict methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817b668d-56d9-40bb-8833-7f2def32e319",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Create pipeline\n",
    "pipe_lr = make_pipeline(StandardScaler(), PCA(n_components-2), LogisticRegression())\n",
    "\n",
    "# Fit\n",
    "pipe_lr.fit(X_train, y_train)\n",
    "y_pred = pipe_lr.predict(X_test)\n",
    "test_acc = pipe_lr.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec5a77e-4e81-4376-bf48-024b281b7ccc",
   "metadata": {},
   "source": [
    "- StandardScaler -> PCA -> LogisiticRegression\n",
    "- fit will pass the data down the pipeline and then the estimator will fit the processed data\n",
    "- predict will pass tha data down the pipeline and then the estimator will predict the processed data\n",
    "\n",
    "<img src=\"Pipeline.png\" width=\"40%\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f8ae6d-83cd-4341-baad-2c0739f94866",
   "metadata": {},
   "source": [
    "### 6.2 Cross-Validation Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b56465c-4384-445b-84af-e9afd10724f9",
   "metadata": {},
   "source": [
    "- Help us to obtain reliable estimates of the model's generalizations performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20d9bc3-b6b7-4e60-9ae9-5ac8a45d27d0",
   "metadata": {},
   "source": [
    "#### 6.2.1 Holdout Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feec31a5-a548-44e5-a02b-73ab7eacceaf",
   "metadata": {},
   "source": [
    "- Using the holdout methods, we split our intial dataset into separate training and test datasets, the former is for model training, and the latter is used to estimate its generalization performance\n",
    "- We are interested i tuning and comparing different parameter settings to further improve the performance for making predictions on unseen data\n",
    "- $\\textbf{Model Selection}$ - refers to a given a classfication problem for which we want to select the optimal values of tuning parameters (hyperparameters)\n",
    "- If we reuse the same test dataset over and over again during model selection, it will beomce part of our training data and thus the model will be more likely to overfit\n",
    "- The best way of using the holdout method is to separate the  data into three parts:\n",
    "    - training dataset - fit the different models\n",
    "    - validation dataset - performance on the validation dataset is used for model selection\n",
    "    - test dataset\n",
    "- The advantage of having a test dataset that the model hasn't seen before during the training and model seleciton steps is that we can obtain a less biased estimate of its ability to generalize  to new data\n",
    "\n",
    "<img src=\"HoldoutValidation.png\" width=\"40%\"/>\n",
    "\n",
    "- Disadvantage is that the performance estimate may be very sensitive to how we partion the dataset into the training and validation subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c85e11-c377-41f8-bb1e-1d4603447e52",
   "metadata": {},
   "source": [
    "#### 6.2.2 K-Fold Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eff8ee0-0195-4a61-b148-fb280df3a42e",
   "metadata": {},
   "source": [
    "- Repeat the holdout method $k$ times on $k$ subsets of the training data\n",
    "- Randomly splut the training dataset into $k$ folds without replacement\n",
    "- $k - 1$ folds (training folds) are used for model training\n",
    "- One folds (test fold) - is used for performance evaluation\n",
    "- This procedure is repeated $k$ times so that we obtain $k$ models and perforamnce estimates\n",
    "- We then calcualte the average performance of the models based on the different, independent test folds to obtain a performance estimate that is less sensitive to the sub-partitioning of the training data compared to the holdout method\n",
    "- Typically, we use k-fold cross-validation for model tuning (finding the optimal hyperparameter values that yield a satisfying generalization performance)\n",
    "- Once we have found satisfactory hyperparameter values, we can retrain the model on the complete dataset and obtain a final performance estimate using the independent test dataset, as providing more training examples to a learnign algorthm usuallly results in a more accurate and robust model\n",
    "\n",
    "<img src=\"KFoldsValidation.png\" width=\"40%\"/>\n",
    "\n",
    "- A good standard value for $k$ in k-fold croos-validation is 10, as it seems to offer the best tradeoff between bias and variance\n",
    "- If we are working with relatively small training sets, it can be useful to increase the number of folds\n",
    "- If we increae $k$, more training data is used in each iteration, which results in a lower pessimistic bias towards estimating the generalization perforamnce by averaging the individual models\n",
    "- Larger $k$ values will increase the runtiume of the cross-validation algorithm and yield estimates with higher variance.\n",
    "- On the other hand, if we are working with large datasetsm we can choose a smaller value for $k$\n",
    "\n",
    "- $\\textbf{Leave-One-Out Cross-Validation (LOOCV)}$\n",
    "    - a special case of k-fold cross-validation.\n",
    "    - We set the number of training folds to the number oftraining examples ($k = n$) so that only one training example is used for testing during each iteration.\n",
    "    - Recommended for working with very small datasets\n",
    " \n",
    "- $\\textbf{Stratified K-Fold Cross-Validation}$\n",
    "    - A slight improvment of standard k-folds\n",
    "    - Can yield better bias and variance estimates, especially in cases of unequal class proportions\n",
    "    - Class label proportions are preserved in each fold to ensure that each fold is representative of the class proportions in the training dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a01694d-bffa-4d0a-a4a7-fa3db1846753",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(estimator-pipe_lr, X=X_train, y=t_train, cv=10, n_jobs=1)\n",
    "# n_jobs = -1 uses all available CPUs on the machine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83133dd9-7012-4614-bea0-11b017bdaf91",
   "metadata": {},
   "source": [
    "### 6.3 Learning and Validation Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac31385-ee0d-4249-9124-4bdeb123782c",
   "metadata": {},
   "source": [
    "- Two tools that can help us improve the performance of a learning algorithm:\n",
    "    - Learning Curves\n",
    "    - Validation Curves\n",
    "\n",
    "- By plotting the model training and validation accuracies as functions of the training dataset size, we can eaisly detectwhether the model suffers from high variance or high bias, and whether the collection of more data could help to address this problem\n",
    "\n",
    "#### Learning Curves\n",
    "\n",
    "<img src=\"CommonModelIssues.png\" width=\"40%\"/>\n",
    "\n",
    "- High bias - the model has both low training and cross-validation accuracy.\n",
    "    - Increase the number of model parameters\n",
    "    - Decrease the degree of regularization\n",
    "- High Variance - large gap between the training and cross-validation accuracy\n",
    "    - Collect more training data (may not help if data is extremely noisy or model is close to optimal)\n",
    "    - Reduce the complexity of the model\n",
    "    - Increase the regularization parameter\n",
    "    - Decrease the number of features via feature selection or feature extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddefa14c-1132-4f9a-afc9-1debf805b3cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "train_sizes, train_scores, test_scores = learning_curve(estimator=pipe_lr, x=X_train, y=y_train, train_sizes = np.linspace(0.1, 1.0, 10), cv=10, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e47dbf4-c626-40c0-a378-0f49d56859a5",
   "metadata": {},
   "source": [
    "#### Validation Curves\n",
    "- Related to learning curves but we vary the values of the model parameters\n",
    "- Ideal range is 0.1 to 1.0 in plot below\n",
    "  \n",
    "<img src=\"ValidationCurve.png\" width=\"40%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f8c040b-859e-4a1a-91a6-0ab2a7825c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve\n",
    "param_range = [0.0001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "train_scores, test_scores = validation_curve(estimator=pipe_lr, X=X_train, y=t_ytrain, param_name='logisiticregression_C', param_range=param_range, cv=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe6b992-1068-4387-bde8-8e0cf87232e2",
   "metadata": {},
   "source": [
    "### 6.4 Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58373a98-48d4-4b9d-ae87-1e3102d9545b",
   "metadata": {},
   "source": [
    "#### 6.4.1 Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ffb0669-3ab3-4b42-a12b-9e2fb43bb3b8",
   "metadata": {},
   "source": [
    "- Popular hyperparameter optimization technique\n",
    "- $\\textbf{Grid Search}$ - Help to improve the performance of a model by finding the optimal combination of hyperparameter values\n",
    "- Specify a list of values for different hyperparameters, and the computer evaluates the model performance for each combination to obtain the optimal combination of values from this set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a5268b-4123-4bfa-92f2-4b21c94d5046",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_seleciton import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "pipe_svc = make_pipeline(StandardScalar(), SVC(random_state=1))\n",
    "param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "param_grid = [{'svc_C': param_range, 'svc_kernel':['linear']}, {'svc_C': param_range, 'svc_gamma':param_range, 'svc_kernel':['rbf']}]\n",
    "gs = GridSearchCV(estimator=pipe_svc, param_grid=param_grid, scoring='accuracy', cv=10, refit=True, n_jobs=-1)\n",
    "gs.fit(X_train, y_train)\n",
    "\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)\n",
    "\n",
    "clf = gs.best_estimator_\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b09842-2f12-499b-ba39-bc58bc164ff7",
   "metadata": {},
   "source": [
    "- grid search is an exhaustive search, it is guaranteed to find the optimal hyperparameters configuration if it is contained in the user-specified parameter grid\n",
    "- Specifying larger hyperparameter grids make grid search very expensive in practive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f048a9-302f-4899-81ec-1e3f940d017e",
   "metadata": {},
   "source": [
    "#### 6.4.2 Randomized Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d69793-f0e5-4c43-93b4-6d4619f5053b",
   "metadata": {},
   "source": [
    "- We draw hyperparameter configurations randomly from distributions (or discrete sets)\n",
    "- It is not exhaustive, but allows use to explore a wider range of hyperparameter value settings in a more cost- and time-effctive manner\n",
    "- Can use distributions for specifying parameter ranges\n",
    "      \n",
    "<img src=\"GridVsRandomSearch.png\" width=\"50%\"/>\n",
    "\n",
    "- Grid search may miss good hyperparameter configurations if the search space is to scarce\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76faec72-bf5a-4a97-9002-e107030fa033",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats\n",
    "param_range = scipy.stats.loguniform(0.0001, 1000.0)\n",
    "\n",
    "np.random_seed(1)\n",
    "param_range.rvs(10)\n",
    "param_grid = [{'svc_C': param_range, 'svc_kernel':['linear']}, {'svc_C': param_range, 'svc_gamma':param_range, 'svc_kernel':['rbf']}]\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "rs = RandomizedSearchCV(estimator=pipe_svc, param_distributions=param_grid,scoring='accuracy', refit=True, n_iter=20, cv=10, random_State=1, n_jobs=-1)\n",
    "rs.fit(X_train, y_trian)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0b3bb5-32e2-43b8-a551-c9214a7a0038",
   "metadata": {},
   "source": [
    "#### 6.4.3 Succesive Halving"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e37b9657-efef-4b82-93cd-35e0852726a3",
   "metadata": {},
   "source": [
    "- Makes finding suitable hyperparameter configuartions mroe efficient\n",
    "- Given a large set of candidate configurations, successively throws out unpromising hyperparameter configurations until only one configuration remains\n",
    "\n",
    "- Successive Haling Procedure:\n",
    "    1. Draw a large set of candidate configurations via radnom sampling\n",
    "    2. Train the models with limited resources, for example, a small subset of the training data\n",
    "    3. Discard the bottom 50 percent based on predictive performance\n",
    "    4. Go back to step 2 with an increase amount of available resources if more than one configuration remains\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e115cb4-1f0c-410f-9111-94faddac57aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import HalvingRandomSearchCV\n",
    "hs = HalvingRandomSearchCV(pipe_svc, param_distribtuions=param_grid, n_candidates='exhaust', resource='n_samples', factor=1.5, random_State=1, n)jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7361a81a-3ab1-4f06-862c-851498cd8bdd",
   "metadata": {},
   "source": [
    "#### 6.4.4 Tree-structued Parzen Estimators (TPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4f5ab5-1ac8-4a9e-b9f1-56700147e1ee",
   "metadata": {},
   "source": [
    "- Bayesian optimization method based on a probabilistic model that is continously updated based on  past hyperparameter evaluations as independent events\n",
    "- hyperopt-sklearn package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ac3a728-fd3b-4236-9070-ed1d9b3828a4",
   "metadata": {},
   "source": [
    "#### 6.4.5 Nested cross-validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b943575-9d29-47f3-8535-915ac8fd81ae",
   "metadata": {},
   "source": [
    "- Using k-fold cross-validation in combination with grid search or randomized search is a useful approach for fine-tuning the performance of a machine learning model by varying its hyperparameters\n",
    "- Nested cross-validation is a recommended approach if we want to select among different machine learning algorithms\n",
    "- $\\textbf{Nested Cross-Validation}$\n",
    "    - We have an outer k-fold cross-validation loop to split the data into training and test folds\n",
    "    - There is an inner loop to select the model using k-fold cross-validation on the training fold\n",
    "    - After model selection, the test fold is then used to evaluate the model performance\n",
    "\n",
    "Below is a 5x2 cross-validation\n",
    "\n",
    "<img src=\"NestedCrossValidation.png\" width=\"40%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e509bee-e857-4916-8d09-734a0e2b0b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n",
    "param_grid = [{'svc_C': param_range, 'svc_kernel': ['linear']},{'svc_C': param_range, 'svc_gamma': param_range, 'svc_kernel': ['rbf']}]\n",
    "gs = GridSearchCV(estimator=pipe_svc, param_grid = param_grid, scoring='accuracy', cv=2)\n",
    "scores = cross_val_score(gs, X_train, y_train, scoring-'accuracy', cv=5)\n",
    "\n",
    "gs = GridSearchCV(estimator=DecisionTreeClassifier(random_state=0), param_grid=[{'max_depth': [1,2,3,4,5,6,7,None]}], scoring='accuracy', cv=2)\n",
    "scores = cross_val_score(gs, X_train, y_train, scoring='accuracy', cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f132db3-78c4-4285-946e-8fa4a0f720e3",
   "metadata": {},
   "source": [
    "### 6.5 Performance Evaluation Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b5203d-7c34-4747-a633-6079272a2386",
   "metadata": {},
   "source": [
    "- Performance metrics that can be used to measure a models relevance:\n",
    "    - Prediction accuracy\n",
    "    - Precision\n",
    "    - Recall\n",
    "    - F1 Score\n",
    "    - Matthews Correlation Coefficient (MCC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be194eb-f14a-45c6-897f-8a66833af596",
   "metadata": {},
   "source": [
    "#### 6.5.1 Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc4d525-3bf1-41bd-aaf0-ecafc0c8fa6b",
   "metadata": {},
   "source": [
    "- Matrix that lays out the performance of a learning algorithm\n",
    "- Square matrix that reports the counts of the true positive (TP), true negative (TN), false positive (FP), and flase negative (FN) predictions of a classifier\n",
    "\n",
    "<img src=\"ConfusionMatrix.png\" width=\"30%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a013514f-9a1b-4306-90b3-df03640e6497",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.meatrix import confusion_matrix\n",
    "pipe_svc.fit(X_train, y_train)\n",
    "y_pred = pipe_svc.predict(X_test)\n",
    "confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\n",
    "print(confmat)\n",
    "#ax.matshow(confmat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab33814-b6df-43e4-93bb-d027d07d6014",
   "metadata": {},
   "source": [
    "- prediction error (ERR) and accuracy (ACC) provide general information about how many examples are misclassified\n",
    "- Error is the sum of the false predicitions divided by the total predicitons\n",
    "    - $ERR = \\frac{FP + FN}{FP + FN + TP + TN}$\n",
    "- Accuracy is calculated from the error:\n",
    "    - $ACC = \\frac{TP + TN}{FP + FN + TP + TN} = 1 - ERR$\n",
    "- True positive rate (TPR) and false positive rate (FPR) are performance metrixs that are especially useful for imbalanced class problems\n",
    "    - $FPR = \\frac{FP}{N} = \\frac{FP}{FP + TN}$\n",
    "    - $TPR = \\frac{TP}{P} = \\frac{TP}{FN + TP}$\n",
    "\n",
    "- $\\textbf{Recall}$ - quantifies how many of the relevant records (positives) are captured as such (true positives)\n",
    "    - $REC = TPR = \\frac{TP}{P} = \\frac{TP}{FN + TP}$\n",
    "- $\\textbf{Precision}$ - quantifies how many of the records predicted as relevant (sum of true and false positives) are actually relevant (true positives) \n",
    "    - $PRE = \\frac{TP}{TP + FP}$\n",
    "\n",
    "- In the case of malignant tumor detecting, optimizing for recall helps with minimizing the chance of not detecting a malignant tumor at the cost of predicting malignant tumors in healthy patients (higher number of FPs)\n",
    "- Optimizing precision emphasizes correctness if we predict that a patient has a malignant tumor, at the cost of missing malignant tumors more frequently (higher number of FNs)\n",
    "\n",
    "- $\\textbf{F1 Score}$ - balances the up- and downsides of optimizing precision and recall, by using the harmonic mean of precision and recall\n",
    "    - $F1 = 2 \\frac{PRE \\times REC}{PRE + REC}$\n",
    " \n",
    "- $\\textbf{Matthews Correlation Coefficient (MCC)}$\n",
    "    - Popular in bilogical research contexts\n",
    "    - $MCC = \\frac{TP \\times TN - FP \\times FN}{\\sqrt{(TP + FP)(TP + FN)(TN + FP)(TN + FN)}}$\n",
    "    - Range is [-1, 1]\n",
    "    - Takes all elements of a confusion matrix\n",
    "    - Harder to interpret than the F1 score, but it is considered the superior metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303d86cf-ee76-405a-aea9-8c8f6f2526ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score, f1_score\n",
    "from sklearn.meatrics import matthews_corrcoef\n",
    "\n",
    "pre_val = precision_score(y_true=y_test, y_pred=y_pred)\n",
    "rec_val = recall_score(y_true=y_test, y_pred=y_pred)\n",
    "f1_val = f1_score(y_true=y_test, y_pred=y_pred)\n",
    "mcc_val = matthews_corrcoef(y_true=y_test, y_pred=y_pred)\n",
    "\n",
    "from sklearm.metrics import make_scorer\n",
    "scorer = make_scorer(f1_score, pos_label=0)\n",
    "gs = GridSearchCV(estimator=pipe_svc, param_grid=param_grid, scoring=scorer, cv = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754a4927-d0c9-4474-b5ba-6ea47ba8c65d",
   "metadata": {},
   "source": [
    "- $\\textbf{Reciever operating characteristic (ROC)}$ - useful graphs to select models for classification baed on their performance with respect to the FPR and TPE, which are computed by shifting the decision threshold of the classifier\n",
    "    - The diagonal can be interpreted as random guessing\n",
    "    - A classificaiton model that falls below the diagonal are considered as worse than random guessing\n",
    "    - A perfect classifier would fall into the top-left corner of the graph with a TPR of 1 and FPR of 0\n",
    "    - Based on the ROC curve we can compute the ROC area under the curve (ROC AUC) to characterize the performance of a classification model\n",
    "- Precision-recall curves - compute for different probability thresholds of a classifier\n",
    "\n",
    "#### Metrics for multiclass classification\n",
    "\n",
    "- All of the previous metrics are specific to binary classification systems, however, scikit-learn implements macro and micro averaging methods to extend those scoring metrics to multiclass problems via one-vs-all (OvA) classification\n",
    "- $\\textbf{Micro-average}$ - calculated from the individual TPs, TNs, FPs, FNs of the system\n",
    "    - $PRE_{micro} = \\frac{ TP_{1} + ... + TP_{k}}{TP_{1} + ... + TP_{k} + FP_{1} + ... + FP_{k}}$\n",
    "    - Useful if we want to weight each instance or prediction equally\n",
    "- $\\textbf{Macro-average}$ - calculated as the average scores of the different systems\n",
    "    - $PRE_{macro} = \\frac{PRE_{1} + ... + PRE_{k}}{k}$\n",
    "    - Weights all classes equally to evaluate overall performance of a classifier with regard to the most frequent class labels\n",
    "    - Used by default if using binary performance metric to evalluate multiclass classification\n",
    "    - Weighted macro-average - calcualted by weighting the score of each class label by the number of true instances when calculating the average\n",
    "    - Useful if dealing with class impalances\n",
    " \n",
    "#### Class Imbalance\n",
    "- Very common in real-world data\n",
    "- When there is a class imbalance it makes sense to focus on other metrics than accuracy when comparing models\n",
    "- How to deal with class imbalance:\n",
    "    - Assign a larger penalty to wrong predictions on the minority class\n",
    "        - set class_weight='balanced'\n",
    "    - upsampling the minority class\n",
    "        - resample function\n",
    "    - downsampling the majority class\n",
    "        - resample function\n",
    "    - Generate synthetic training examples\n",
    "        - Synthetic minority over-sampling technique (SMOTE) \n",
    "- No universally best solution across different problems - try out different solutions and choose the best one\n",
    "   \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d714880-c210-4810-a383-0abcabc2d369",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Chapter 7: Ensemble Learing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0591dfea-2a31-4e44-8fea-f42f583bf0fa",
   "metadata": {},
   "source": [
    "- $\\textbf{Ensemble Methods}$ - combine different classifiers into a meta-classifier that has better generalization performance than each individual classifier alone\n",
    "- Ensemble Methods:\n",
    "    - Majority Voting (Binary Classification) or Plurality Voting (Multiclass Classification)\n",
    "\n",
    "- Train $m$ different classifiers ($C_{1}$,$C_{2}$,...,$C_{m}$)\n",
    "- The ensemble can be built from different classificaiton algorithms\n",
    "- Can also use the same base classificaiton algorithm, fitting different subsets of the training dataset (Random Forest)\n",
    "  \n",
    "<img src=\"MajorityVoting.png\" width=\"30%\"/>\n",
    "\n",
    "- Why do ensemble methods work better than individual classes\n",
    "- Assume:\n",
    "    - All $n$-base classifiers for a binary classification task have an equal error rate, $\\epsilon$\n",
    "    - The classifiers are independent\n",
    "    - Error rates are not correlated\n",
    "- Error probability of an ensemvle of base classifiers as a probability mass function of a binomial distribution:\n",
    "    - $P(y \\geq k) = \\sum\\limits_{k}^{n} \\left\\langle \\begin{matrix} n \\\\ k \\end{matrix}\\right\\rangle  \\epsilon^{k} (1 - \\epsilon)^{n-k}  = \\epsilon_{ensemble}$\n",
    "\n",
    "<img src=\"EnsembleError.png\" width=\"30%\"/>\n",
    "\n",
    "The error probabilty of an ensemble is always better than the error of an individual base classifier, as long as the base classifiers perform better than random guestting ($\\epsilon \\lt 0.5$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d4d821-570f-4610-9ef0-6e07b066ca8e",
   "metadata": {},
   "source": [
    "#### 7.1 Majority Vote"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28da3a1e-f57b-4d6e-8ead-86a94d1f8577",
   "metadata": {},
   "source": [
    "- Combine the predicted class labels of each individual classifier, $C_{j}$, and select the class label, $\\hat{y}$, that received the most votes\n",
    "    - $\\hat{y}$ = mode[$C_{1}(x)$, $C_{2}(x)$, ..., $C_{M}(x)$]\n",
    " \n",
    "- Weighted majority vote:\n",
    "    - $\\hat{y} = $ arg max$_{i} \\sum\\limits_{j=1}^{m} w_{j}\\chi_{A}(C_{j}(x) = i)$\n",
    "    - $w_{j}$ - weight associated with a base classifier, $C_{j}$\n",
    "    - $A$ is the set of unique class labels\n",
    "    - $\\chi_{A}$ is the characteristic function or indicator function (returns 1 if the predicted class of the $j$th classifier matches $i$)\n",
    "    - For equal weights we end up with the previous equation\n",
    "    - Given probabilities:\n",
    "        - $\\hat{y} = $ arg max$_{i} \\sum\\limits_{j=1}^{m} w_{j}p_{ij}$\n",
    "        - $p_{ij}$ - predicted probability of the $j$th classifier for class label $i$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "57be3391-316e-4952-be2b-e2474df4e3df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.92 (+/- 0.15) [Logisitic Regression]\n",
      "ROC AUC: 0.87 (+/- 0.18) [Decision Tree]\n",
      "ROC AUC: 0.85 (+/- 0.13) [KNN]\n",
      "ROC AUC: 0.98 (+/- 0.05) [Majority Voting]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Load the data and encode the class labels\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data[50:, [1, 2]], iris.target[50:]\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "# Split the Iris examples into 50% training and 50% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=1, stratify=y)\n",
    "\n",
    "# Evaluate the model performance of each classifier via 10-fold cross-validation\n",
    "clf1 = LogisticRegression(penalty='l2', C=0.001, solver='lbfgs', random_state=1)\n",
    "pipe1 = Pipeline([['sc', StandardScaler()], ['clf', clf1]])\n",
    "clf2 = DecisionTreeClassifier(max_depth=1, criterion='entropy', random_state=0)\n",
    "clf3 = KNeighborsClassifier(n_neighbors=1, p=2, metric='minkowski')\n",
    "pipe3 = Pipeline([['sc', StandardScaler()],['clf', clf3]])\n",
    "\n",
    "# Define Majority Voting\n",
    "mv_clf = VotingClassifier(estimators=[('lr', pipe1), ('dt', clf2), ('knn', pipe3)],voting='soft',weights=[1, 1, 1])\n",
    "clf_labels = ['Logisitic Regression', 'Decision Tree', 'KNN', 'Majority Voting']\n",
    "for clf, label in zip([pipe1, clf2, pipe3, mv_clf], clf_labels):\n",
    "    scores = cross_val_score(estimator=clf, X=X_train, y=y_train, cv=10, scoring='roc_auc')\n",
    "    print(f'ROC AUC: {scores.mean():.2f} (+/- {scores.std():.2f}) [{label}]')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549ae570-7c4d-400b-93f1-1be1bb09e8d8",
   "metadata": {},
   "source": [
    "<img src=\"MajorityVoteComp.png\" width=\"30%\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c615ad54-a606-4824-a514-01dd3f848503",
   "metadata": {},
   "outputs": [],
   "source": [
    "#mv_clf.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0c5ccd46-3be4-43fa-ae9e-5b54447774b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.983 +/- 0.05 {'dt__max_depth': 1, 'lr__clf__C': 0.001}\n",
      "0.983 +/- 0.05 {'dt__max_depth': 1, 'lr__clf__C': 0.1}\n",
      "0.967 +/- 0.10 {'dt__max_depth': 1, 'lr__clf__C': 100.0}\n",
      "0.983 +/- 0.05 {'dt__max_depth': 2, 'lr__clf__C': 0.001}\n",
      "0.983 +/- 0.05 {'dt__max_depth': 2, 'lr__clf__C': 0.1}\n",
      "0.967 +/- 0.10 {'dt__max_depth': 2, 'lr__clf__C': 100.0}\n",
      "ROC AUC : 0.983\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Grid Search to tune hyperparameters\n",
    "params = {'dt__max_depth': [1, 2], 'lr__clf__C': [0.001, 0.1, 100.0]}\n",
    "grid = GridSearchCV(estimator=mv_clf, param_grid=params, cv=10, scoring='roc_auc')\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Get ROC AUC score\n",
    "for r, _ in enumerate(grid.cv_results_['mean_test_score']):\n",
    "    mean_score = grid.cv_results_['mean_test_score'][r]\n",
    "    std_dev = grid.cv_results_['std_test_score'][r]\n",
    "    params = grid.cv_results_['params'][r]\n",
    "    print(f'{mean_score:.3f} +/- {std_dev:.2f} {params}')\n",
    "print(f'ROC AUC : {grid.best_score_:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67167b7-572d-4f3b-beab-2b77813ff322",
   "metadata": {},
   "source": [
    "#### 7.2 Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fd1d32-4709-48c6-8477-497afc507798",
   "metadata": {},
   "source": [
    "- Closely related to Majority Vote Classifier\n",
    "- Instead of using the same training dataset to fit the individual classifiers inthe ensemble, we draw bootstrap samples (random with replacement) from the initial training dataset\n",
    "- Also known as $\\textbf{Bootstrap Aggregating}$\n",
    "- Can be an effective approach to reduce the variance of a model\n",
    "- Infeffective in reducing model bias\n",
    "- Typically want to perform bagging on an ensemble of classifiers with low bias (unpruned decision trees)\n",
    "\n",
    "<img src=\"Bagging.png\" width=\"30%\"/>\n",
    "\n",
    "<img src=\"Bagging2.png\" width=\"50%\"/>\n",
    "\n",
    "- Above there are 7 training istances that are sampled randomly with replacement in each round of bagging\n",
    "- Each bootstrap sample is then used to fit a classifier, $C_{j}$ (typically an unpruned decision tree)\n",
    "- Each classifier receives a random subset of examples from the training dataset denoted via bagging as Bagging round 1, Bagging Round 2, ...\n",
    "- Each subset contains a certain portion of duplicates and some of the original examples don't appear inthe dataset\n",
    "- Once the individual classifiers are fit to the bootstrap samples the predictions are combined using majority voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e15f65-59c5-4ca8-b447-7d27ae720185",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "tree = DecisionTreeClassifier(criterion='entropy', random_state=1, max_depth=None)\n",
    "bag = BaggingClassifier(base_estimator=tree, n_estimators=500, max_samples=1.0, max_ffeatures=1.0, bootstrap=True, bootstrap_features=false, n_jobs=-1, random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "496630a3-9854-4f6e-95d2-0b94f5bdb7af",
   "metadata": {},
   "source": [
    "#### 7.3 Adaptive Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14377be1-c07d-4788-9587-ce4e2ebcb93b",
   "metadata": {},
   "source": [
    "- The ensemble consists of very simple base classifiers, weak learners\n",
    "- $\\textbf{weak learners}$ - only have a slight performance advantage over random guessing (decision tree stump)\n",
    "- Focus on training examples that are hard to classify, that is, to let the weak learners subsequently learn from misclassified training examples to improve the performance of the ensemble\n",
    "- Algorithm:\n",
    "    1. Draw a random subset (sample) of training examples, $d_{1}$, without replacement from the training dataset, $D$, to train a weak learner, $C_{1}$\n",
    "    2. Draw a second random training subset, $d_{2}$, without replacement from the training dataset and add 50 percent of the examples that were previously misclassified to train a weak learner, $C_{2}$\n",
    "    3. Find the training examples, $d_{3}$, in the training dataset, $D$, which $C_{1}$ and $C_{2}$ disagree upon, to train a third weak learner, $C_{3}$\n",
    "    4. Combine the weak learners $C_{1}$, $C_{2}$, and $C_{3}$ via majority voting\n",
    "\n",
    "- Boosting can lead to a decrease in bias as well as variance compared to bagging models\n",
    "- Known for their high variance (overfit the data)\n",
    "- AdaBoost uses the complete training dataset to train the weak learners, where the training examples are reweighted in each iteration to build a strong classifier that learns from the mistakes of the previous weak learners in the ensemble\n",
    "\n",
    "<img src=\"AdaBoost.png\" width=\"50%\"/>\n",
    "\n",
    "1. All training examples are assigned equal weights. Based on this dataset, we train a decision stump that tries to classify the examples of the two classes, as well as possible minimizing the loss function\n",
    "2. We assign a larger weight to the two previously misclassified examples (circles). We lower the weight of the correctly classified examples. The next decision stump will now be more focused on the training examplesthat have the largest weights (ones that are supposedly harder to classify)\n",
    "3. 3 different examples form the circle class were misclassified in subfigure 2 so they are assigned a larger weight\n",
    "4. Assuming AdaBoost ensemle only consists of three rounds of boosting, we then combine the three weak learners trained on different reweighted training subsets by a weighted majority vote\n",
    "\n",
    "- AdaBoost Algorithm:\n",
    "    1. Set the weight vector, w, to uniform weights, where $\\sum\\limits_{i} w_{i} = 1$\n",
    "    2. For $j$ in $m$ boosting rounds, do the following:\n",
    "        1.  Train a weighted weak learner: $C_{j} = train(X, y, w)$\n",
    "        2.  Predict class labels: $\\hat{y} = predict(C_{j},\\textbf{X})$\n",
    "        3.  Compute the weighted error rate: $ \\epsilon = \\textbf{w} \\cdot (\\hat{y} \\neq y)$ (binary vector where a 1 if prediction is incorrect else 0)\n",
    "        4.  Compute the coefficient: $a_{j} = 0.5\\log{\\frac{1-\\epsilon}{\\epsilon}}$\n",
    "        5.  Update the weights: $\\textbf{w} \\coloneqq \\textbf{w} \\times \\exp(-\\alpha_{j} \\times \\hat{y} \\times y)$\n",
    "        6.  Normalize the weights to sum to 1: $\\textbf{w} \\coloneqq \\frac{\\textbf{w}}{\\sum\\limits_{i} w_{i}}$\n",
    "    3. Compute the final prediction: $\\hat{y} = \\left(\\sum\\limits_{j=1}^{m} (\\alpha_{j} \\times predict(C_{j}, \\textbf{X})) \\gt 0 \\right)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f71c14-3756-4cc2-a141-98cf977822b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "tree = DecisionTreeClassifier(criterion='entropy', random_state=1, max_depth=1)\n",
    "ada = AdaBoostClassifier(base_estimator=tree, n_estimators=500, learning_rate=0.1, random_state=1)\n",
    "ada.ada.fit(X_train, y_train)\n",
    "y_test_pred = ada.predict(X_test)\n",
    "ada_test = accuracy(y_test, y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f61a08-395c-4143-807c-6673e1bfc9ee",
   "metadata": {},
   "source": [
    "#### 7.4 Gradient Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5091c5a5-36ad-4b85-b057-a83ee4aef3f1",
   "metadata": {},
   "source": [
    "- Another variance of boosting - successively training weak learners to create a strong ensemble\n",
    "- Forms the basis popular machince learning algorithms such as XGBoost\n",
    "- Fits gradient decision trees in an iterative fashion using prediction errors\n",
    "- The trees are usually deeper than decision tree stumps and typically a maximum depth of 3 to 6 (8 to 64 leaf nodes)\n",
    "- Does not use the prediciton errors for assigning sample weights; they are used directly to form the target variable for fitting the next tree\n",
    "- Instead of having an individual weighting term for each tree, it uses a global learning rate that is the same for each tree\n",
    "- In essence, gradient boosting builds a series of trees, where each tree is fit on the error - the difference between the label and the predicted value - of the previous tree. In each round, the tree enemble improves as we are nudging each tree more in the right direction via small updates. These updates are based on a loss gradient\n",
    "  \n",
    "- Gradient Boosting Algorithm:\n",
    "    1. Initialize a model to return a constant prediction value. For this, we use a decision tree root node. We denote the value returned by the tree as $\\hat{y}$, and we find this value by minimizing a differentiable loss function $L$ that we will define later. Here, $n$ refers to the $n$ training examples in out dataset:\n",
    "        - $F_{0}(x) = $ arg min$_{\\hat{y}} \\sum\\limits_{i=1}^{n} L(y_{i}, \\hat{y})$\n",
    "    2. For each tree $m = 1, ..., M$, where $M$, is a user-specified total number of trees, we carry out the following computations outlines in steps 2a to 2d below:\n",
    "        1. Compute the difference between a predicited value $F(x_{i}) = \\hat{y}_{i}$ and the class label $y_{i}$. This value is somteimtes called the pseudo-response or pseudo-residual. More formally, we can write this pseudo-residual as the negative gradient of the loss function with respect to the predicted values. (Note: that in the notation below $F(x)$ is the prediction of the previous tree, $F_{m-1}(x)$. So, in the first round, this refers to the constant value from the tree (single leaf node) from step 1.\n",
    "            - $r_{im} = - \\left [ \\frac{\\delta L(y_{i}, F(x_{i}))}{\\delta F(x_{i})} \\right ]_{F(x)=F_{m-1}(x)}$ for $i = 1, ..., n$\n",
    "        2. Fit a tree to the pseudo-residuals $r_{im}$. We use the notation $R_{jm}$ to denote the $j = 1 ... J_{m}$ leaf nodes of the resulting tree in iteration $m$.\n",
    "        3. For each leaf node $R_{jm}$, we compute the following output value\n",
    "            - $ \\gamma_{jm} = $ arg min$_{\\gamma} \\sum\\limits_{x_{i} \\epsilon R_{jm}} L(y_{i}, F_{m-1}(x_{i}) + \\gamma$)\n",
    "            - $\\gamma_{jm}$ is computed byminimizing the loss function\n",
    "        4. Update the model by adding the output values $\\gamma$ to the previous tree:\n",
    "            -  $F_{m}(x) = F_{m-1}(x) + \\eta \\gamma_{m}$\n",
    "            -  Scale the predicted values of the current tree $\\gamma_{m}$ by the learning rate, $\\eta$ (typically between 0.01 and 1), before adding it to the previous tree $F_{m-1}$\n",
    "          \n",
    "#### XGBoost\n",
    "- Extreme gradient boosting\n",
    "- it has been the winning solution for many Kaggle competitions\n",
    "- Other popular implementations:\n",
    "    - LightGBM\n",
    "    - CatBoost\n",
    "    - HistGradientBoostingClassifier in sklearn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a74d35-f7b5-441d-83d6-5d56a6063721",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "model = xgb.XGBClassifier(n_estimators=1000, learning_rate=0.01, max_depth=4, random_state=1, use_label_encoder=False)\n",
    "gbm = model.fit(X_train, y_train)\n",
    "y_test_pred = gbm.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d176a485-e74a-4d2c-bc9c-9cd4e79348ab",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Chapter 8: Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc12ec4d-101d-4d68-9e52-c23913fdbd47",
   "metadata": {},
   "source": [
    "### 8.1 IMDb Movie Review Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aa8e81f-5255-4897-8757-0f5a052b9a29",
   "metadata": {},
   "source": [
    "- $\\textbf{Opinion Mining}$ (sentiment analysis) - subdispline of natural language processing (NLP) concerned with analyzing the sentiment of documents\n",
    "- Popular task of is the classification of documents based on the expressed opinions or emotions of the authors with reguard to a particular topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "59330b24-63be-4a0b-b0c7-f122cf8b6d10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/j7/r32w1g2s1453yzksbljngwdc0000gq/T/ipykernel_42557/3920617591.py:17: FutureWarning: The frame.append method is deprecated and will be removed from pandas in a future version. Use pandas.concat instead.\n",
      "  df = df.append([[txt, labels[l]]], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0% [##############################] 100% | ETA: 00:00:00\n",
      "Total time elapsed: 00:00:48\n"
     ]
    }
   ],
   "source": [
    "import pyprind\n",
    "import pandas as pd\n",
    "import os\n",
    "import sys\n",
    "\n",
    "basepath = 'data/aclImdb'\n",
    "labels = {'pos':1, 'neg': 0}\n",
    "pbar = pyprind.ProgBar(50000, stream=sys.stdout)\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for s in ('test', 'train'):\n",
    "    for l in ('pos', 'neg'):\n",
    "        path = os.path.join(basepath, s, l)\n",
    "        for file in sorted(os.listdir(path)):\n",
    "            with open(os.path.join(path, file), 'r', encoding='utf-8') as infile:\n",
    "                txt = infile.read()\n",
    "            df = df.append([[txt, labels[l]]], ignore_index=True)\n",
    "            pbar.update()\n",
    "df.columns = ['review', 'sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ccdd62f7-ab40-4ab0-a626-3fd24da3568d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "df = df.reindex(np.random.permutation(df.index))\n",
    "df.to_csv('movie_data.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ab5dbbdc-8e34-4c4a-ad70-6f1873f6c7ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>In 1974, the teenager Martha Moxley (Maggie Gr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>OK... so... I really like Kris Kristofferson a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>***SPOILER*** Do not read this, if you think a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hi for all the people who have seen this wonde...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>I recently bought the DVD, forgetting just how...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  sentiment\n",
       "0  In 1974, the teenager Martha Moxley (Maggie Gr...          1\n",
       "1  OK... so... I really like Kris Kristofferson a...          0\n",
       "2  ***SPOILER*** Do not read this, if you think a...          0\n",
       "3  hi for all the people who have seen this wonde...          1\n",
       "4  I recently bought the DVD, forgetting just how...          0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('movie_data.csv', encoding='utf-8')\n",
    "df = df.rename(columns={'0':'review', '1': 'sentiment'})\n",
    "df .head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c68155ad-9e04-4403-869d-25680dc752f0",
   "metadata": {},
   "source": [
    "### 8.2 Bag-of-words Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a464935d-1834-45dc-86ea-c5de3cde4603",
   "metadata": {},
   "source": [
    "- $\\textbf{bag-of-words model}$ - allows us to represent text as numerical feature vectors\n",
    "- Algorithm:\n",
    "    1. We create a vocabulary of unique tokens - for example, words - from the entire documents\n",
    "    2. We construct a feature vector from each document that contains the counts of how often each word occurs in the particular documenta\n",
    "- The unique words in each document represent only a small subset of all the words in the bag-of-words vocabulary, the feature vectos will be sparse\n",
    "- $\\textbf{Raw Term Frequencies}$ - the number of times a term, $t$, occurs in a document, $d$\n",
    "    - $tf(t, d)$ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8b3c3b67-02dc-46c6-965a-2a207397f66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the': 6, 'sun': 4, 'is': 1, 'shining': 3, 'weather': 8, 'sweet': 5, 'and': 0, 'one': 2, 'two': 7}\n",
      "[[0 1 0 1 1 0 1 0 0]\n",
      " [0 1 0 0 0 1 1 0 1]\n",
      " [2 3 2 1 1 1 2 1 1]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "count = CountVectorizer(ngram_range=(1,1))\n",
    "docs = np.array(['The sun is shining','The weather is sweet','The sun is shining, the weather is sweet, and one and one is two'])\n",
    "\n",
    "bag = count.fit_transform(docs)\n",
    "print(count.vocabulary_)\n",
    "print(bag.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed0fbe6-da07-4ba5-95e4-9552e4965ce5",
   "metadata": {},
   "source": [
    "#### 8.2.1 N-Gram Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c620570c-1ce1-4be7-91d8-77a43085ea6a",
   "metadata": {},
   "source": [
    "- The sequence of items in the bag-of-words model is also called the 1-gram or unigram model - each item or token in the vocabulary represents a single word\n",
    "- The contiguous sequence of items in NLP - words, letters, or symbols- are also called n-grams\n",
    "- The choice of the number $n$ depends on the particular application\n",
    "- For example:\n",
    "    - 1-gram: 'the', 'sun', 'is', 'shining'\n",
    "    - 2-gram: 'the sun', 'sun is', 'is shining'    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b694b8-8ac5-418f-9bd7-29bd78cbb14a",
   "metadata": {},
   "source": [
    "#### 8.2.2 Word Relevancy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf252d5-858c-4007-96de-5ff8a4a56ed2",
   "metadata": {},
   "source": [
    "- $\\textbf{Term Frequency-Inverse Document Frequency (td-idf)}$ - technique used to downweight frequently occuring words in the feature vectors\n",
    "    - $tf$-$idf(t, d) = tf(t, d) \\times idf(t, d)$\n",
    "    - $idf(t, d) = \\log{\\frac{n_{d}}{1 + df(d, t)}}$\n",
    "    - $m+{d}$ is the total number of documents\n",
    "    - $df(d, t)$ is the number ofdocuments, $d$, that contain the term $t$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ff8d10b2-8812-4200-a31d-01371465de09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.   0.43 0.   0.56 0.56 0.   0.43 0.   0.  ]\n",
      " [0.   0.43 0.   0.   0.   0.56 0.43 0.   0.56]\n",
      " [0.5  0.45 0.5  0.19 0.19 0.19 0.3  0.25 0.19]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf = TfidfTransformer(use_idf=True, norm='l2', smooth_idf=True)\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "print(tfidf.fit_transform(count.fit_transform(docs)).toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6609737-8ee7-48aa-80a7-8f37b2c9dc9b",
   "metadata": {},
   "source": [
    "#### Cleaning text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "f3c6790e-d7f8-429d-bcbc-fafc353ad1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocessor(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(/:\\)|\\(|D|P)', text)\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower())) + ' '.join(emoticons).replace('-', '')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d669f755-2b10-4bfe-b5ff-07ff9f1a1f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['review'] = df['review'].apply(preprocessor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158cd829-84ea-4026-9118-f45baaaabd7f",
   "metadata": {},
   "source": [
    "#### Process Documents into Tokens\n",
    "- $\\textbf{Word Stemming}$ - process of transfroming a word into its root form (Natural Language Toolkit (NLTK))\n",
    "- $\\textbf{Stop Word Removal}$ - remove words that are extremely common in all sorts of texts and probably bear no useful inforamtion that can be used to distinguish between different classes of documents (is, and, had, like)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5512dd0e-4bca-4234-811b-2f3aa63fa360",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer(text):\n",
    "    return text.split()\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "def tokenizer_porter(text):\n",
    "    return [porter.steam(word) for word in text.split()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7855059c-d5e2-4e91-a586-ce50a92229ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "743d5128-6dc1-4817-8cac-136c8db0f050",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "def removeStopWords(text):\n",
    "    return [w for w in tokenizer_porter(text)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a318fa-7bf1-42bf-9b54-71c0b31d572b",
   "metadata": {},
   "source": [
    "### 8.3 Logisitic Regression for Document Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "81d92188-1bff-4417-9b09-174168a0b370",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df.loc[:25000, 'review'].values\n",
    "y_train = df.loc[:25000, 'sentiment'].values\n",
    "X_test = df.loc[25000:, 'review'].values\n",
    "y_test = df.loc[25000:, 'sentiment'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b10756ac-2e7c-4915-b20c-51cd08e4819c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(strip_accents=None, lowercase=False, preprocessor=None)\n",
    "small_param_grid = [\n",
    "    {\n",
    "        'vect__ngram_range': [(1,1)],\n",
    "        'vect__stop_words': [None],\n",
    "        'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "        'clf__penalty': ['l2'],\n",
    "        'clf__C': [1.0, 10.0]\n",
    "    },\n",
    "    {\n",
    "        'vect__ngram_range': [(1,1)],\n",
    "        'vect__stop_words': [stop, None],\n",
    "        'vect__tokenizer': [tokenizer, tokenizer_porter],\n",
    "        'vect__use_idf': [False],\n",
    "        'vect__norm': [None],\n",
    "        'clf__penalty': ['l2'],\n",
    "        'clf__C': [1.0, 10.0]\n",
    "    },\n",
    "]\n",
    "\n",
    "lr_tfidf = Pipeline([('vect', tfidf), ('clf', LogisticRegression(solver='liblinear'))])\n",
    "\n",
    "gs_lr_tfidf = GridSearchCV(lr_tfidf, small_param_grid, scoring='accuracy', cv=5, verbose=2, n_jobs=1)\n",
    "\n",
    "gs_lr_tfidf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "d95179a6-17d5-4159-9d7a-dcd6de4a3ce6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter set: {'clf__C': 10.0, 'clf__penalty': 'l2', 'vect__ngram_range': (1, 1), 'vect__stop_words': None, 'vect__tokenizer': <function tokenizer at 0x12a1b48b0>}\n",
      "CV Accuracy: 0.897\n",
      "Test Accuracy: 0.898\n"
     ]
    }
   ],
   "source": [
    "print(f'Best parameter set: {gs_lr_tfidf.best_params_}')\n",
    "print(f'CV Accuracy: {gs_lr_tfidf.best_score_:.3f}')\n",
    "clf = gs_lr_tfidf.best_estimator_\n",
    "print(f'Test Accuracy: {clf.score(X_test, y_test):.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95636d5b-d834-4bb4-91a7-dcdc5b0162b6",
   "metadata": {},
   "source": [
    "- Naive Bayes classifier\n",
    "    - easy to implement\n",
    "    - computationally efficient\n",
    "    - Tends to perform particularly well on relatively small dataset\n",
    "    - popular in applications of email spam filtering "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5041c2c8-1ab6-4ba4-9f33-f38527d1a5d0",
   "metadata": {},
   "source": [
    "### 8.4 Online Algorithms and Out-of-core Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294505fb-51ca-4b0f-b03d-1c99aaca0d6e",
   "metadata": {},
   "source": [
    "- $\\textbf{Out-of-core learning}$ - allows us to work with such large datasets by fitting the classifier incrementally on smaller batches of a dataset\n",
    "- $\\textbf{Stochastic Gradient Descent}$ - optimization algorithm that updates the model using one example at a time\n",
    "- Out-of-core learning is very memroy efficient, and take significantly less time than a grid search hyperparameter tuning method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "83e8b689-55a1-4d54-97ec-3907e2980a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "# Tokenize a document\n",
    "def tokenizer(text):\n",
    "    text = re.sub('<[^>]*>', '', text)\n",
    "    emoticons = re.findall('(?::|;|=)(?:-)?(/:\\)|\\(|D|P)', text)\n",
    "    text = (re.sub('[\\W]+', ' ', text.lower())) + ' '.join(emoticons).replace('-', '')\n",
    "    tokenized = [w for w in text.split() if w not in stop]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "62359912-690b-4097-8556-f25f555355e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reads in and returns one document at a time\n",
    "def stream_docs(path):\n",
    "    with open(path, 'r', encoding='utf-8') as csv:\n",
    "        next(csv)\n",
    "        for line in csv:\n",
    "            text, label = line[:-3], int(line[-2])\n",
    "            yield text, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "8c8de962-7556-4a5f-87b0-a6bd13430a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a document stream from the stream_docs function and return a particular number of documents specified by the size parameter\n",
    "def get_minibatch(doc_stream, size):\n",
    "    docs, y = [], []\n",
    "    try:\n",
    "        for _ in range(size):\n",
    "            text, label = next(doc_stream)\n",
    "            docs.append(text)\n",
    "            y.append(label)\n",
    "    except StopIteration:\n",
    "        return None, None\n",
    "    return docs, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0d0905ab-d7fd-4501-b21e-1a4b73c20857",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "vect = HashingVectorizer(decode_error='ignore', n_features=2**21, preprocessor=None, tokenizer=tokenizer)\n",
    "clf = SGDClassifier(loss='log', random_state=1)\n",
    "doc_stream = stream_docs(path='movie_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77956de-d59f-4877-8abc-6f285984c219",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyprind\n",
    "pbar = pyprind.ProgBar(45)\n",
    "classes = np.array([0, 1])\n",
    "for _ in range(45):\n",
    "    X_train, y_train = get_minibatch(doc_stream, size=1000)\n",
    "    if not X_train:\n",
    "        break\n",
    "    X_train = vect.transform(X_train)\n",
    "    clf.partial_fit(X_train, y_train, classes=classes)\n",
    "    pbar.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03184cd-17c3-4a4d-b8e5-1ca5f3100c99",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = get_minibatch(doc_stream, size=5000)\n",
    "X_test = vect.transform(X_test)\n",
    "print(f'Accuracy: {clf.score(X_test, y_test):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3be8d9-9900-42b7-9f61-479674784255",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf - clf.partial_fit(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9fea34c-b483-4f51-9381-212b1109b45e",
   "metadata": {},
   "source": [
    "### 8.5 Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e602f8f-7e4a-430b-8eeb-2368cb819800",
   "metadata": {},
   "source": [
    "- $\\textbf{Topic Modeling}$ - the broad task of assigning topics to unlabeled text documents\n",
    "- Can consider topic modeling as a clustering task, a subcategory of unsupervised learning\n",
    "- $\\textbf{Latent Dirichlet Allocation (LDA)}$ - popular topic modelling technique\n",
    "- LDA is a generative probabilistic model that tries to find groups of words that appear frequently together across different documents.\n",
    "- Given a bag-of-wordsmatrix as input LDA decomposes it into two new matrices:\n",
    "    - A document-to-topic matrix\n",
    "    - A word-to-topic matrix\n",
    "- The number of topics is a hyperparameter of LDA that has to be specified manually\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "d10d1e17-02d8-4864-99f0-a92c91692885",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('movie_data.csv', encoding='utf-8')\n",
    "df = df.rename(columns={'0':'review', '1':'sentiment'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "28894e04-7e48-46fc-890a-0d8a233f8e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count = CountVectorizer(stop_words='english', max_df=.1, max_features=5000)\n",
    "X = count.fit_transform(df['review'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "5970f8f6-f42e-42b4-b10a-2981ccef4ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda = LatentDirichletAllocation(n_components=10, random_state=123, learning_method='batch', n_jobs=-1)\n",
    "X_topics = lda.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "b6cd67f7-be71-4850-94c2-b43a08764470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 5000)\n"
     ]
    }
   ],
   "source": [
    "print(lda.components_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "c898dcca-9e93-49c3-8ee2-c8833727bf8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 1:\n",
      "worst minutes awful script stupid\n",
      "Topic 2:\n",
      "family mother father children girl\n",
      "Topic 3:\n",
      "american war dvd music tv\n",
      "Topic 4:\n",
      "human audience cinema art sense\n",
      "Topic 5:\n",
      "police guy car dead murder\n",
      "Topic 6:\n",
      "horror house sex girl woman\n",
      "Topic 7:\n",
      "role performance comedy actor performances\n",
      "Topic 8:\n",
      "series episode war episodes tv\n",
      "Topic 9:\n",
      "book version original read novel\n",
      "Topic 10:\n",
      "action fight guy guys cool\n"
     ]
    }
   ],
   "source": [
    "n_top_words = 5\n",
    "feature_names = count.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(lda.components_):\n",
    "    print(f'Topic {(topic_idx + 1)}:')\n",
    "    print(' '.join([feature_names[i] for i in topic.argsort()[:-n_top_words - 1:-1]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ae4abb-48ba-451c-b567-750f036e5155",
   "metadata": {},
   "source": [
    "LDA Identified the following topics:\n",
    "1. Generally bad movies\n",
    "2. Movies about families\n",
    "3. War movies\n",
    "4. Art movies\n",
    "5. Crime Movies\n",
    "6. horro Movies\n",
    "7. Comedy movie reviews\n",
    "8. Movies somehow related to TV shows\n",
    "9. Movies based on books\n",
    "10. Action Movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "e1ab49f5-f758-495a-a904-c4fbf3fb53b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Horror movie #1:\n",
      "House of Dracula works from the same basic premise as House of Frankenstein from the year before; namely that Universal's three most famous monsters; Dracula, Frankenstein's Monster and The Wolf Man are appearing in the movie together. Naturally, the film is rather messy therefore, but the fact that ...\n",
      "\n",
      "Horror movie #2:\n",
      "Okay, what the hell kind of TRASH have I been watching now? \"The Witches' Mountain\" has got to be one of the most incoherent and insane Spanish exploitation flicks ever and yet, at the same time, it's also strangely compelling. There's absolutely nothing that makes sense here and I even doubt there  ...\n",
      "\n",
      "Horror movie #3:\n",
      "<br /><br />Horror movie time, Japanese style. Uzumaki/Spiral was a total freakfest from start to finish. A fun freakfest at that, but at times it was a tad too reliant on kitsch rather than the horror. The story is difficult to summarize succinctly: a carefree, normal teenage girl starts coming fac ...\n"
     ]
    }
   ],
   "source": [
    "horror = X_topics[:, 5].argsort()[::-1]\n",
    "for iter_idx, movie_idx in enumerate(horror[:3]):\n",
    "    print(f'\\nHorror movie #{(iter_idx + 1)}:')\n",
    "    print(df['review'][movie_idx][:300],'...')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936eb030-5974-4ffe-8ab0-6e421b7bfaaf",
   "metadata": {},
   "source": [
    "## Chapter 9: Regression Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2570ec-f4e4-4f49-963e-d0c2b27c75a3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2857951e-836b-4eab-b0de-41ca4afc67ce",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "810f9898-5600-4993-a674-cbce6e191058",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc292613-e437-4526-bb7b-c74e39993fcc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5069b0f2-070d-44df-bd56-304c93649bf2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0ebcc2ab-4705-478e-9c12-beeeaeca450e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d017de1-65bf-44c3-995b-fd85a715aea6",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a30260b-b096-4d76-ba25-3d12c0cdabff",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6c50b82b-de9a-412e-ab04-8c5c7b01c260",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1cf6651a-e515-4fff-9d66-70c2125e4cbe",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18579166-1c42-4bfa-a807-f875929b7586",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1292d57b-5be8-447a-b238-cd5529cde95f",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "59003c51-38a9-4b92-856a-43049422077a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "adaefde8-474e-4964-b1ae-9d49b89893cb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f55f440f-7221-448f-8129-d387f0b288b1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ae9f6ec3-3cc8-4823-bd16-82b5e019dcf1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
