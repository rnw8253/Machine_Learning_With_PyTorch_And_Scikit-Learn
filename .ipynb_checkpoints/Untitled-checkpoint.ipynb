{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59943c35-3a8f-485b-80ac-4fdd12aab51c",
   "metadata": {},
   "source": [
    "# Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476313bb-db2e-459c-a31f-41ccc019622b",
   "metadata": {},
   "source": [
    "## Chapter 1\n",
    "\n",
    "### Types of Machine Learning\n",
    "- $\\textbf{Supervised Learning}$\n",
    "    - Labeled Data\n",
    "    - Direct Feedback\n",
    "    - Predict outcome/future\n",
    "    - Subcategories:\n",
    "        - Classification - predict categorical class\n",
    "        - Regression - predict continous outcomes\n",
    "- $\\textbf{Unsupervised Learning}$\n",
    "    - No labels/targets\n",
    "    - No feedback\n",
    "    - Find hidden structure in data\n",
    "    - Subcategories:\n",
    "        - Clustering - organize data into meaningful subgroups\n",
    "        - Dimensionality Reduction - compress the data onto a smaller dimensional subspace while retaining most of the relevant information\n",
    "- $\\textbf{Reinforcement Learning}$\n",
    "    - Decision process\n",
    "    - Reward system\n",
    "    - Learn series of actions\n",
    "    - Agent improves its performance based on interactions with the environment\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "234f93fc-6250-4ed3-95cf-3567ddb07f77",
   "metadata": {},
   "source": [
    "## Chapter 2\n",
    "<img src=\"machineLearningRoadmap.png\" width=\"50%\"/>\n",
    "\n",
    "### Perceptron\n",
    "\n",
    "- Rosenblatt's thresholded perceptron model mimics the function of a single neuron in the brain\n",
    "- Use perceptron implementation to create a decision boundary to classify data into categories (binary)\n",
    "- $\\textbf{OvR}$: one-versus-rest - technique that allows us to extend any binary classifier to multi-class problems\n",
    "- Takes a combination of certain inputs and a corresponding weight vector to predict the outcome\n",
    "\n",
    "- Learning rule:\n",
    "    1. Initialize the weights and bias unit to 0 or small random numbers\n",
    "    2. For each training example, $x^{(i)}$:\n",
    "        - Compute the output value, $\\hat{y}^{(i)}$ (predicted class label)\n",
    "        - Update the weights and bias unit\n",
    "            - $w_{j} := w_{j} + \\Delta w_{j}$ where $\\Delta w_{j} = \\eta(y^{(i)} - \\hat{y}^{(i)})x_{j}^{(i)}$\n",
    "            - $b := b + \\Delta b$ where $\\Delta b = \\eta(y^{(i)} - \\hat{y}^{(i)})$\n",
    "            - $\\eta$ is the learning rate\n",
    "\n",
    "\n",
    "<img src=\"PerceptronWeightsAndBias.png\" width=\"50%\"/>\n",
    "\n",
    "### Adaptive Linear Neurons (Adaline)\n",
    "\n",
    "<img src=\"AdalineWeightsAndBias.png\" width=\"50%\"/>\n",
    "\n",
    "- Weights are updated based on a linear activation function rather than a unit step function like in the perceptron\n",
    "- Threshold function is still used to make the final prediction\n",
    "- $\\textbf{Objective Function}$ - optimized during the learning process; often a loss or cost function that we want to minimize\n",
    "- Adaline loss function is the mean squared error (MSE) between the calculated outcome and the true class label:\n",
    "    - $L(w, b) = \\frac{1}{n} \\sum\\limits_{i=1}^{n} (y^{(i)} - \\sigma(z^{(i)}))^{2}$\n",
    "- The loss function becomes differentiable and convex allowing us to use the powerful $\\textbf{gradient descent}$ optimization algorithm\n",
    "    - Step in the opposite direction of the gradient\n",
    "    - $\\Delta \\mathbf{w} = -\\eta \\nabla_{w} L(\\textbf{w}, b) = \\eta \\frac{2}{n} \\sum\\limits_{i} ( y^{(i)} - \\sigma(z^{(i)}))x_{j}^{(i)}$ \n",
    "    - $\\Delta b = -\\eta \\nabla_{b} L(\\textbf{w}, b) = \\eta \\frac{2}{n} \\sum\\limits_{i} ( y^{(i)} - \\sigma(z^{(i)}))$\n",
    " \n",
    "<img src=\"GradientDescent.png\" width=\"40%\"/>\n",
    "\n",
    "- Gradient descent converges much more quickly if we implement feature scaling\n",
    "- $\\textbf{Standardization}$:\n",
    "    - Shifts the mean of each feature so that it is centered at zero and each feature has a standard deviation of 1 (unit variance)\n",
    "    - $x^{'}_{j} = \\frac{x_{j} - \\mu_{j}}{\\sigma_{j}}$\n",
    "    - where, $x_{j}$ is a vector consisting of the jth feature values of all training examples\n",
    "- $\\textbf{Stochastic Gradient Descent}$:\n",
    "    - Instead of calculating the gradient from the entire data set we randomly pick one data point\n",
    "    - Updates gradient much more frequently and although it is an approximation of the actual gradient it converges much faster\n",
    "    - Fixed learning rate is replace by an adaptive learning rate that decreases over time: $\\frac{c_{1}}{[number \\ of \\ iterations] + c_{2}}$\n",
    "    - $\\textbf{Online Learning}$ - model is trained on the fly as new training data arrives and data can be discarded after updating the model if storage space is an issue\n",
    "- $\\textbf{Mini-batch Gradient Descent}$:\n",
    "    - Middle ground of SGD and full-batch gradient descent\n",
    "    - Apply full-batch to a subset of the data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b3d557-02da-4015-8df4-503d2e4276fd",
   "metadata": {},
   "source": [
    "## Chapter 3\n",
    "\n",
    "### Classifiers \n",
    "\n",
    "- No free lunch theorem - no single classifier works best across all possible scenarios\n",
    "- It is recommended that you compare the performance of at least a handful of different learning algorithms to select the best model for the particular problem. These may differ in:\n",
    "    - The number of features or examples\n",
    "    - The amount of noise in the dataset\n",
    "    - Whether the classes are linearly separable\n",
    " \n",
    "- The five main steps involved in training a supervised machine learning algorithm:\n",
    "    1. Selecting features and collecting labeled training examples\n",
    "    2. Choosing a performance metric\n",
    "    3. Choosing a learning algorithm and training a model\n",
    "    4. Evaluating the performance of the model\n",
    "    5. Changing the settings of the algorithm and tuning the model\n",
    "\n",
    "### Logistic Regression\n",
    "\n",
    "- Linear model used for binary classification\n",
    "- Very easy to implement and performs very well on linearly separable classes\n",
    "- The activation function becomes the sigmoid function which maps any number back to [0, 1] range\n",
    "- The output of the sigmoid function is the interpreted as the probability of a particular example belonging to a class\n",
    "- This probability is often just as interesting as the predicted class label\n",
    "- It is recommended to use mroe advanced approaches than regular SGD (newton-cg, lbfgs, liblinear, sag, saga)\n",
    "\n",
    "<img src=\"LogisticRegression.png\" width=\"50%\"/>\n",
    "\n",
    "### Overfitting\n",
    "\n",
    "- $\\textbf{Overfitting}$ - a model performs well on training data but does not generalize well to unseen data (test data)\n",
    "- It is said the model suffers from having high variance which can be caused by having too many parameters, leading to a model that is too complex for the underlying data\n",
    "- $\\textbf{Underfitting}$ - (high bias) the model is not complex enough to capture the pattern in the training data well and suffers from low performance on unseen data\n",
    "\n",
    "<img src=\"Overfitting.png\" width=\"50%\"/>\n",
    "\n",
    "- $\\textbf{bias-variance tradeoff}$:\n",
    "    - high variance refers to overfitting\n",
    "    - high bias refers to underfitting\n",
    " \n",
    "- One way of finding a good bias-variance tradeoff is to tune the complexity of the model via regularization\n",
    "- $\\textbf{Regularization}$ - introduce additional information to penalize extreme parameter (weight) values. Useful method for handling:\n",
    "    - colinearity - high correlation among features\n",
    "    - filtering out noise from the data\n",
    "    - preventing overfitting\n",
    "- $\\textbf{L2 regularization}$ - (L2 shrinkage / weight decay) most common form of regularization. Adds the following term to the loss function:\n",
    "    - $\\frac{\\lambda}{2n} ||\\textbf{w} ||^{2} = \\frac{\\lambda}{2n} \\sum\\limits_{j=1}^{m} w_{j}^{2}$\n",
    "    - Regularization parameter, $\\lambda$ - used to control how closely we fit the traing data, while keeping the weights small\n",
    "    - Parameter C implemented in Logistic Regression class in skikit-learn comes from SVM and is inversely proportional to the $\\lambda$\n",
    "    - If the regularization strength is set too high the weight coefficients approach zero and the model can perform poorly due to underfitting\n",
    " \n",
    "### Support Vector Machines (SVM)\n",
    "\n",
    "### Decision Tree\n",
    "\n",
    "### Random Forest\n",
    "\n",
    "### K-nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f75acd2-a913-4d07-8772-64f02c33ec9b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
